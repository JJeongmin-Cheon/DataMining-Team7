{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242e0e0a",
   "metadata": {},
   "source": [
    "### 문화 행사 정보 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e99c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5597 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ 'cp949' 인코딩 시도 중...\n",
      "❌ 실패 (cp949): 'cp949' codec can't decode byte 0x98 in position 8: illegal multibyte sequence\n",
      "⏳ 'utf-8' 인코딩 시도 중...\n",
      "✅ 파일 읽기 성공: utf-8\n",
      "총 5597개의 행이 있습니다.\n",
      "사용할 위도: 위도(Y좌표), 경도: 경도(X좌표)\n",
      "사용할 날짜: 날짜/시간, 시간: 날짜/시간\n",
      "🗺️ 좌표 → 행정동 정보 변환 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5597/5597 [16:47<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 최종 CSV 저장 완료: 서울시_문화행사_위치정보_시간포함.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 카카오 REST API 키 입력 (https://developers.kakao.com)\n",
    "KAKAO_API_KEY = \"api_key\"  # 여기에 본인의 REST API 키 입력\n",
    "\n",
    "CSV_FILE_PATH = \"서울시_문화행사_정보.csv\"  # 원본 CSV 경로\n",
    "\n",
    "# 1. 한글 인코딩 오류 방지를 위한 robust CSV 읽기 함수\n",
    "def read_csv_robust(file_path):\n",
    "    encodings = ['cp949', 'utf-8', 'euc-kr', 'utf-8-sig']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"⏳ '{encoding}' 인코딩 시도 중...\")\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"✅ 파일 읽기 성공: {encoding}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 실패 ({encoding}): {e}\")\n",
    "    raise ValueError(\"⚠️ 모든 인코딩 시도 실패\")\n",
    "\n",
    "# 2. 카카오 reverse geocoding 함수\n",
    "def get_admin_code_kakao(lat, lng, api_key):\n",
    "    url = \"https://dapi.kakao.com/v2/local/geo/coord2regioncode.json\"\n",
    "    headers = {\"Authorization\": f\"KakaoAK {api_key}\"}\n",
    "    params = {\"x\": lng, \"y\": lat}\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, params=params, timeout=5)\n",
    "        res.raise_for_status()\n",
    "        data = res.json()\n",
    "        for doc in data.get(\"documents\", []):\n",
    "            if doc['region_type'] == 'H':  # 행정동 우선\n",
    "                return {\n",
    "                    '행정동코드': doc['code'],\n",
    "                    '시도': doc['region_1depth_name'],\n",
    "                    '시군구': doc['region_2depth_name'],\n",
    "                    '행정동': doc['region_3depth_name'],\n",
    "                    '성공': True,\n",
    "                    '오류': None\n",
    "                }\n",
    "        return {'행정동코드': None, '시도': None, '시군구': None, '행정동': None, '성공': False, '오류': 'No region_type H'}\n",
    "    except Exception as e:\n",
    "        return {'행정동코드': None, '시도': None, '시군구': None, '행정동': None, '성공': False, '오류': str(e)}\n",
    "\n",
    "# 3. 메인 실행 함수\n",
    "def main():\n",
    "    df = read_csv_robust(CSV_FILE_PATH)\n",
    "    print(f\"총 {len(df)}개의 행이 있습니다.\")\n",
    "\n",
    "    # 위도/경도 컬럼 자동 탐지\n",
    "    lat_col = next((col for col in df.columns if \"위도\" in col or \"Y\" in col or \"lat\" in col.lower()), None)\n",
    "    lng_col = next((col for col in df.columns if \"경도\" in col or \"X\" in col or \"lon\" in col.lower()), None)\n",
    "    if lat_col is None or lng_col is None:\n",
    "        raise ValueError(\"⚠️ 위도/경도 컬럼을 찾을 수 없습니다.\")\n",
    "\n",
    "    # 날짜/시간 컬럼 자동 탐지\n",
    "    date_col = next((col for col in df.columns if \"일\" in col or \"날짜\" in col), None)\n",
    "    time_col = next((col for col in df.columns if \"시\" in col or \"시간\" in col), None)\n",
    "\n",
    "    print(f\"사용할 위도: {lat_col}, 경도: {lng_col}\")\n",
    "    print(f\"사용할 날짜: {date_col}, 시간: {time_col}\")\n",
    "\n",
    "    # 결과 리스트 초기화\n",
    "    행정동코드, 시도, 시군구, 행정동, 성공, 오류 = [], [], [], [], [], []\n",
    "\n",
    "    print(\"🗺️ 좌표 → 행정동 정보 변환 중...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        lat, lng = row[lat_col], row[lng_col]\n",
    "        if pd.isna(lat) or pd.isna(lng):\n",
    "            행정동코드.append(None); 시도.append(None); 시군구.append(None); 행정동.append(None)\n",
    "            성공.append(False); 오류.append(\"Invalid coord\")\n",
    "            continue\n",
    "        result = get_admin_code_kakao(lat, lng, KAKAO_API_KEY)\n",
    "        행정동코드.append(result['행정동코드'])\n",
    "        시도.append(result['시도'])\n",
    "        시군구.append(result['시군구'])\n",
    "        행정동.append(result['행정동'])\n",
    "        성공.append(result['성공'])\n",
    "        오류.append(result['오류'])\n",
    "        time.sleep(0.1)  # 카카오 초당 10회 제한\n",
    "\n",
    "    # 결과 결합\n",
    "    df[\"행정동코드\"] = 행정동코드\n",
    "    df[\"시도\"] = 시도\n",
    "    df[\"시군구\"] = 시군구\n",
    "    df[\"행정동\"] = 행정동\n",
    "    df[\"geocoding_success\"] = 성공\n",
    "    df[\"geocoding_error\"] = 오류\n",
    "\n",
    "    # 필요한 컬럼만 저장\n",
    "    save_cols = [col for col in [date_col, time_col, lat_col, lng_col, \"행정동코드\", \"시도\", \"시군구\", \"행정동\"] if col]\n",
    "    df_result = df[save_cols]\n",
    "\n",
    "    output_file = \"서울시_문화행사_위치정보_시간포함.csv\"\n",
    "    df_result.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n✅ 최종 CSV 저장 완료: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025fda78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로드된 데이터: 5597개\n",
      "컬럼: ['날짜/시간', '날짜/시간.1', '위도(Y좌표)', '경도(X좌표)', '행정동코드', '시도', '시군구', '행정동']\n",
      "                   날짜/시간                날짜/시간.1    위도(Y좌표)     경도(X좌표)  \\\n",
      "0  2025-12-18~2025-12-21  2025-12-18~2025-12-21  37.511824  127.059159   \n",
      "1  2025-12-06~2025-12-06  2025-12-06~2025-12-06  37.549906  126.945534   \n",
      "2  2025-10-18~2025-10-19  2025-10-18~2025-10-19  37.457066  126.896037   \n",
      "3  2025-10-03~2025-10-12  2025-10-03~2025-10-12  37.529365  127.073978   \n",
      "4  2025-09-27~2025-09-28  2025-09-27~2025-09-28  37.641994  127.077437   \n",
      "\n",
      "          행정동코드     시도  시군구   행정동  \n",
      "0  1.168058e+09  서울특별시  강남구  삼성1동  \n",
      "1  1.144060e+09  서울특별시  마포구   대흥동  \n",
      "2  1.154567e+09  서울특별시  금천구  시흥1동  \n",
      "3  1.121584e+09  서울특별시  광진구  자양3동  \n",
      "4  1.135061e+09  서울특별시  노원구  하계1동  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 가장 완성된 문화행사 데이터 로드\n",
    "culture_df = pd.read_csv(\"서울시_문화행사_위치정보_시간포함.csv\", encoding='utf-8-sig')\n",
    "print(f\"로드된 데이터: {len(culture_df)}개\")\n",
    "print(f\"컬럼: {culture_df.columns.tolist()}\")\n",
    "\n",
    "# 데이터 확인\n",
    "print(culture_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09af43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감지된 인코딩: {'encoding': 'UTF-8-SIG', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# 파일의 실제 인코딩 확인\n",
    "def detect_file_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result\n",
    "\n",
    "# 인코딩 확인\n",
    "encoding_info = detect_file_encoding(\"서울시_문화행사_정보.csv\")\n",
    "print(f\"감지된 인코딩: {encoding_info}\")\n",
    "\n",
    "# 감지된 인코딩으로 파일 읽기\n",
    "df = pd.read_csv(\"서울시_문화행사_정보.csv\", encoding=encoding_info['encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7102676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8-SIG 인코딩으로 원본 파일 읽는 중...\n",
      "원본 데이터: 5597개\n",
      "컬럼명: ['분류', '자치구', '공연/행사명', '날짜/시간', '장소', '기관명', '이용대상', '이용요금', '출연자정보', '프로그램소개', '기타내용', '홈페이지?주소', '대표이미지', '신청일', '시민/기관', '시작일', '종료일', '테마분류', '위도(Y좌표)', '경도(X좌표)', '유무료', '문화포털상세URL']\n",
      "\n",
      "분류 컬럼 샘플: ['전시/미술', '클래식', '축제-문화/예술', '축제-문화/예술', '축제-문화/예술']\n",
      "유무료 컬럼 샘플: ['유료', '유료', '무료', '무료', '무료']\n",
      "위도, 경도 기준으로 분류/유무료 정보 매칭 중...\n",
      "\n",
      "매칭 성공: 5596개\n",
      "\n",
      "분류별 통계:\n",
      "교육/체험       2240\n",
      "클래식         1183\n",
      "축제-문화/예술     431\n",
      "전시/미술        411\n",
      "콘서트          351\n",
      "연극           264\n",
      "영화           190\n",
      "기타           165\n",
      "국악           101\n",
      "뮤지컬/오페라       92\n",
      "Name: 분류, dtype: int64\n",
      "\n",
      "유무료 통계:\n",
      "무료    3719\n",
      "유료    1877\n",
      "         1\n",
      "Name: 유무료, dtype: int64\n",
      "\n",
      "✅ 최종 완성된 파일이 '서울시_문화행사_최종완성.csv'로 저장되었습니다.\n",
      "\n",
      "최종 데이터 미리보기:\n",
      "                   날짜/시간        분류 유무료         행정동코드     시도   시군구   행정동\n",
      "0  2025-12-18~2025-12-21     전시/미술  유료  1.168058e+09  서울특별시   강남구  삼성1동\n",
      "1  2025-12-06~2025-12-06       클래식  유료  1.144060e+09  서울특별시   마포구   대흥동\n",
      "2  2025-10-18~2025-10-19  축제-문화/예술  무료  1.154567e+09  서울특별시   금천구  시흥1동\n",
      "3  2025-10-03~2025-10-12  축제-문화/예술  무료  1.121584e+09  서울특별시   광진구  자양3동\n",
      "4  2025-09-27~2025-09-28  축제-문화/예술  무료  1.135061e+09  서울특별시   노원구  하계1동\n",
      "5  2025-09-26~2025-09-26       콘서트  유료  1.123054e+09  서울특별시  동대문구   제기동\n",
      "6  2025-09-20~2025-09-20  축제-문화/예술  무료  1.121584e+09  서울특별시   광진구  자양3동\n",
      "7  2025-09-17~2025-09-21     전시/미술  무료  1.150063e+09  서울특별시   강서구  방화1동\n",
      "8  2025-08-21~2025-08-21       클래식  유료  1.156055e+09  서울특별시  영등포구  당산1동\n",
      "9  2025-08-07~2025-08-10     전시/미술  유료  1.168058e+09  서울특별시   강남구  삼성1동\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 원본 파일을 UTF-8-SIG 인코딩으로 읽기\n",
    "print(\"UTF-8-SIG 인코딩으로 원본 파일 읽는 중...\")\n",
    "original_df = pd.read_csv(\"서울시_문화행사_정보.csv\", encoding='UTF-8-SIG')\n",
    "\n",
    "print(f\"원본 데이터: {len(original_df)}개\")\n",
    "print(f\"컬럼명: {original_df.columns.tolist()}\")\n",
    "\n",
    "# 2. 분류와 유무료 컬럼 확인\n",
    "print(f\"\\n분류 컬럼 샘플: {original_df.iloc[:5, 0].tolist()}\")  # 0번째 컬럼\n",
    "print(f\"유무료 컬럼 샘플: {original_df.iloc[:5, 20].tolist()}\")  # 20번째 컬럼\n",
    "\n",
    "# 3. 현재 문화행사 데이터 읽기\n",
    "culture_df = pd.read_csv(\"서울시_문화행사_위치정보_시간포함.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# 4. 원본에서 필요한 정보 추출\n",
    "original_info = pd.DataFrame({\n",
    "    'category': original_df.iloc[:, 0],      # 분류 (0번째 컬럼)\n",
    "    'free_paid': original_df.iloc[:, 20],    # 유무료 (20번째 컬럼)\n",
    "    'latitude': original_df.iloc[:, 18],     # 위도 (18번째 컬럼)\n",
    "    'longitude': original_df.iloc[:, 19]     # 경도 (19번째 컬럼)\n",
    "})\n",
    "\n",
    "# 5. 매칭하여 분류/유무료 정보 추가\n",
    "culture_df['분류'] = ''\n",
    "culture_df['유무료'] = ''\n",
    "\n",
    "match_count = 0\n",
    "\n",
    "print(\"위도, 경도 기준으로 분류/유무료 정보 매칭 중...\")\n",
    "\n",
    "for idx, row in culture_df.iterrows():\n",
    "    current_lat = row['위도(Y좌표)']\n",
    "    current_lng = row['경도(X좌표)']\n",
    "    \n",
    "    # 매칭되는 원본 데이터 찾기\n",
    "    matching_rows = original_info[\n",
    "        (abs(original_info['latitude'] - current_lat) < 0.00001) &\n",
    "        (abs(original_info['longitude'] - current_lng) < 0.00001)\n",
    "    ]\n",
    "    \n",
    "    if len(matching_rows) > 0:\n",
    "        match = matching_rows.iloc[0]\n",
    "        culture_df.at[idx, '분류'] = str(match['category'])\n",
    "        culture_df.at[idx, '유무료'] = str(match['free_paid'])\n",
    "        match_count += 1\n",
    "\n",
    "print(f\"\\n매칭 성공: {match_count}개\")\n",
    "\n",
    "# 6. 결과 확인\n",
    "print(f\"\\n분류별 통계:\")\n",
    "category_counts = culture_df['분류'].value_counts()\n",
    "print(category_counts.head(10))\n",
    "\n",
    "print(f\"\\n유무료 통계:\")\n",
    "fee_counts = culture_df['유무료'].value_counts()\n",
    "print(fee_counts)\n",
    "\n",
    "# 7. 최종 파일 저장\n",
    "output_file = \"서울시_문화행사_최종완성.csv\"\n",
    "culture_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n✅ 최종 완성된 파일이 '{output_file}'로 저장되었습니다.\")\n",
    "\n",
    "# 8. 결과 미리보기\n",
    "print(f\"\\n최종 데이터 미리보기:\")\n",
    "print(culture_df[['날짜/시간', '분류', '유무료', '행정동코드', '시도', '시군구', '행정동']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef16105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 디렉토리의 모든 파일:\n",
      "  - .ipynb_checkpoints\n",
      "  - data merging.ipynb\n",
      "  - 병합된_데이터셋_문화행사_상세.csv\n",
      "  - 서울시_문화행사_위치정보_시간포함.csv\n",
      "  - 서울시_문화행사_정보.csv\n",
      "  - 서울시_문화행사_최종완성.csv\n",
      "\n",
      "문화행사 관련 파일: ['병합된_데이터셋_문화행사_상세.csv', '서울시_문화행사_위치정보_시간포함.csv', '서울시_문화행사_정보.csv', '서울시_문화행사_최종완성.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 모든 파일 목록 확인\n",
    "all_files = os.listdir('.')\n",
    "print(\"현재 디렉토리의 모든 파일:\")\n",
    "for file in all_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# 문화행사 관련 파일 찾기\n",
    "culture_files = [f for f in all_files if '문화' in f or 'culture' in f.lower()]\n",
    "print(f\"\\n문화행사 관련 파일: {culture_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4328170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 디렉토리의 모든 파일:\n",
      "  - .ipynb_checkpoints\n",
      "  - data merging.ipynb\n",
      "  - merged_holidays.csv\n",
      "  - 병합된_데이터셋_문화행사_상세.csv\n",
      "  - 서울시_문화행사_위치정보_시간포함.csv\n",
      "  - 서울시_문화행사_정보.csv\n",
      "  - 서울시_문화행사_최종완성_수정.csv\n",
      "\n",
      "CSV 파일들: ['merged_holidays.csv', '병합된_데이터셋_문화행사_상세.csv', '서울시_문화행사_위치정보_시간포함.csv', '서울시_문화행사_정보.csv', '서울시_문화행사_최종완성_수정.csv']\n",
      "문화행사 관련 파일: ['병합된_데이터셋_문화행사_상세.csv', '서울시_문화행사_위치정보_시간포함.csv', '서울시_문화행사_정보.csv', '서울시_문화행사_최종완성_수정.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 현재 디렉토리의 모든 파일 확인\n",
    "all_files = os.listdir('.')\n",
    "print(\"현재 디렉토리의 모든 파일:\")\n",
    "for file in all_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# CSV 파일만 필터링\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "print(f\"\\nCSV 파일들: {csv_files}\")\n",
    "\n",
    "# 문화행사 관련 파일 찾기\n",
    "culture_files = [f for f in all_files if '문화' in f or 'culture' in f.lower()]\n",
    "print(f\"문화행사 관련 파일: {culture_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c539bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "local people dong 데이터: 5840개 행\n",
      "문화행사 데이터: 5597개 행사\n",
      "local people dong에 있는 고유 행정동코드: 16개\n",
      "필터링된 문화행사 데이터: 627개 (원본: 5597개)\n",
      "날짜 범위 파싱 및 일별 레코드 생성 중...\n",
      "생성된 일별 문화행사 레코드: 11047개\n",
      "날짜 범위 필터링 후: 7925개\n",
      "데이터 병합 수행 중...\n",
      "\n",
      "📊 병합 결과:\n",
      "전체 레코드: 5,840개\n",
      "문화행사가 있는 레코드: 2,319개 (39.71%)\n",
      "총 문화행사 발생: 7,925건\n",
      "\n",
      "📈 문화행사 분류별 상위 10개:\n",
      "  전시/미술: 790건\n",
      "  클래식: 168건\n",
      "  교육/체험: 134건\n",
      "  교육/체험|전시/미술: 79건\n",
      "  교육/체험|연극: 71건\n",
      "  연극|교육/체험: 66건\n",
      "  축제-시민화합: 66건\n",
      "  연극: 59건\n",
      "  연극|전시/미술: 51건\n",
      "  교육/체험|뮤지컬/오페라: 50건\n",
      "\n",
      "💰 유무료 통계:\n",
      "  무료: 1292건\n",
      "  유료: 481건\n",
      "  무료|유료: 412건\n",
      "  유료|무료: 134건\n",
      "\n",
      "💾 병합된 데이터가 'LOCAL_PEOPLE_DONG_with_culture.csv'로 저장되었습니다.\n",
      "\n",
      "📋 새로 추가된 문화행사 관련 컬럼:\n",
      "  ['has_culture_event', 'culture_event_count', '분류', '유무료']\n",
      "\n",
      "📋 최종 데이터 컬럼:\n",
      "['STDR_DE_ID', 'TMZON_PD_SE', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'date', 'culture_event_count', '분류', '유무료', 'has_culture_event']\n",
      "\n",
      "📋 문화행사가 있는 샘플 데이터:\n",
      "     STDR_DE_ID  ADSTRD_CODE_SE  TOT_LVPOP_CO  culture_event_count  \\\n",
      "480    20240531        11110615   127308.8305                    1   \n",
      "481    20240531        11110650    38217.7573                    1   \n",
      "496    20240601        11110615   107692.2846                    4   \n",
      "497    20240601        11110650    39200.5202                    1   \n",
      "498    20240601        11140550    68181.7737                    1   \n",
      "\n",
      "                 분류    유무료  \n",
      "480             콘서트     무료  \n",
      "481              연극     무료  \n",
      "496  교육/체험|축제-문화/예술  유료|무료  \n",
      "497              연극     무료  \n",
      "498              기타     무료  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_date_range(date_str):\n",
    "    \"\"\"\n",
    "    날짜 범위 문자열을 파싱하여 시작일과 종료일을 반환\n",
    "    예: \"2025-06-28~2025-06-28\" -> (datetime(2025,6,28), datetime(2025,6,28))\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        date_str = str(date_str).strip()\n",
    "        if '~' in date_str:\n",
    "            start_str, end_str = date_str.split('~')\n",
    "            start_date = pd.to_datetime(start_str.strip())\n",
    "            end_date = pd.to_datetime(end_str.strip())\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            single_date = pd.to_datetime(date_str)\n",
    "            return single_date, single_date\n",
    "    except Exception as e:\n",
    "        print(f\"날짜 파싱 오류: {date_str} - {e}\")\n",
    "        return None, None\n",
    "\n",
    "def merge_culture_with_local_people():\n",
    "    \"\"\"\n",
    "    문화행사 데이터를 local people dong 데이터셋과 날짜 범위 기반으로 병합\n",
    "    \"\"\"\n",
    "    print(\"데이터 로드 중...\")\n",
    "    \n",
    "    # 1. local people dong 데이터 로드\n",
    "    local_people_df = pd.read_csv(\"LOCAL_PEOPLE_DONG_202405_202504_filtered_max.csv\")\n",
    "    print(f\"local people dong 데이터: {len(local_people_df)}개 행\")\n",
    "    \n",
    "    # 2. 문화행사 데이터 로드\n",
    "    culture_df = pd.read_csv(\"서울시_문화행사_최종완성_수정.csv\", encoding='utf-8-sig')\n",
    "    print(f\"문화행사 데이터: {len(culture_df)}개 행사\")\n",
    "    \n",
    "    # 3. local people dong 데이터의 날짜 컬럼을 datetime으로 변환\n",
    "    local_people_df['date'] = pd.to_datetime(local_people_df['STDR_DE_ID'], format='%Y%m%d')\n",
    "    \n",
    "    # 4. 행정동코드 형식 통일 (8자리)\n",
    "    local_people_df['admin_code_8'] = local_people_df['ADSTRD_CODE_SE'].astype(str).str[:8]\n",
    "    culture_df['admin_code_8'] = culture_df['행정동코드'].astype(str).str[:8]\n",
    "    \n",
    "    # 5. local people dong에 있는 행정동코드만 추출 (효율성을 위해)\n",
    "    valid_admin_codes = set(local_people_df['admin_code_8'].unique())\n",
    "    print(f\"local people dong에 있는 고유 행정동코드: {len(valid_admin_codes)}개\")\n",
    "    \n",
    "    # 6. 문화행사 데이터를 valid_admin_codes로 필터링\n",
    "    culture_filtered = culture_df[culture_df['admin_code_8'].isin(valid_admin_codes)]\n",
    "    print(f\"필터링된 문화행사 데이터: {len(culture_filtered)}개 (원본: {len(culture_df)}개)\")\n",
    "    \n",
    "    if len(culture_filtered) == 0:\n",
    "        print(\"⚠️ local people dong과 매칭되는 문화행사 데이터가 없습니다.\")\n",
    "        # 기본 컬럼들을 추가\n",
    "        local_people_df['has_culture_event'] = 0\n",
    "        local_people_df['culture_event_count'] = 0\n",
    "        local_people_df['분류'] = ''\n",
    "        local_people_df['유무료'] = ''\n",
    "        return local_people_df\n",
    "    \n",
    "    # 7. 날짜 범위 기반 매칭을 위한 일별 레코드 생성\n",
    "    print(\"날짜 범위 파싱 및 일별 레코드 생성 중...\")\n",
    "    daily_culture_records = []\n",
    "    \n",
    "    for idx, culture_row in culture_filtered.iterrows():\n",
    "        start_date, end_date = parse_date_range(culture_row['날짜/시간'])\n",
    "        \n",
    "        if start_date is None or end_date is None:\n",
    "            continue\n",
    "            \n",
    "        admin_code = culture_row['admin_code_8']\n",
    "        \n",
    "        # 해당 기간의 모든 날짜에 대해 레코드 생성\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            daily_culture_records.append({\n",
    "                'date': current_date,\n",
    "                'admin_code_8': admin_code,\n",
    "                '분류': culture_row.get('분류', ''),\n",
    "                '유무료': culture_row.get('유무료', ''),\n",
    "                'has_culture_event': 1\n",
    "            })\n",
    "            current_date += timedelta(days=1)\n",
    "    \n",
    "    # 8. 일별 문화행사 DataFrame 생성\n",
    "    daily_culture_df = pd.DataFrame(daily_culture_records)\n",
    "    \n",
    "    if len(daily_culture_df) == 0:\n",
    "        print(\"⚠️ 매칭 가능한 문화행사 데이터가 없습니다.\")\n",
    "        local_people_df['has_culture_event'] = 0\n",
    "        local_people_df['culture_event_count'] = 0\n",
    "        local_people_df['분류'] = ''\n",
    "        local_people_df['유무료'] = ''\n",
    "        return local_people_df\n",
    "    \n",
    "    print(f\"생성된 일별 문화행사 레코드: {len(daily_culture_df)}개\")\n",
    "    \n",
    "    # 9. local people dong의 날짜 범위 내에 있는 문화행사만 필터링\n",
    "    min_date = local_people_df['date'].min()\n",
    "    max_date = local_people_df['date'].max()\n",
    "    daily_culture_df = daily_culture_df[\n",
    "        (daily_culture_df['date'] >= min_date) & \n",
    "        (daily_culture_df['date'] <= max_date)\n",
    "    ]\n",
    "    print(f\"날짜 범위 필터링 후: {len(daily_culture_df)}개\")\n",
    "    \n",
    "    # 10. 날짜와 행정동코드별로 문화행사 집계\n",
    "    culture_aggregated = daily_culture_df.groupby(['date', 'admin_code_8']).agg({\n",
    "        'has_culture_event': 'sum',  # 해당 날짜/지역의 문화행사 개수\n",
    "        '분류': lambda x: '|'.join(x.unique()),  # 문화행사 분류들 (중복 제거)\n",
    "        '유무료': lambda x: '|'.join(x.unique())  # 유무료 정보들 (중복 제거)\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 11. 컬럼명 정리\n",
    "    culture_aggregated.rename(columns={\n",
    "        'has_culture_event': 'culture_event_count'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # 12. local people dong 데이터와 병합\n",
    "    print(\"데이터 병합 수행 중...\")\n",
    "    merged_df = local_people_df.merge(\n",
    "        culture_aggregated,\n",
    "        left_on=['date', 'admin_code_8'],\n",
    "        right_on=['date', 'admin_code_8'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 13. 문화행사가 없는 경우 기본값으로 채우기\n",
    "    merged_df['culture_event_count'] = merged_df['culture_event_count'].fillna(0).astype(int)\n",
    "    merged_df['has_culture_event'] = (merged_df['culture_event_count'] > 0).astype(int)\n",
    "    merged_df['분류'] = merged_df['분류'].fillna('')  # 빈 문자열로 채움\n",
    "    merged_df['유무료'] = merged_df['유무료'].fillna('')  # 빈 문자열로 채움\n",
    "    \n",
    "    # 14. 임시 컬럼 제거\n",
    "    merged_df = merged_df.drop('admin_code_8', axis=1)\n",
    "    \n",
    "    # 15. 결과 통계\n",
    "    total_records = len(merged_df)\n",
    "    records_with_events = len(merged_df[merged_df['has_culture_event'] == 1])\n",
    "    total_events = merged_df['culture_event_count'].sum()\n",
    "    \n",
    "    print(f\"\\n📊 병합 결과:\")\n",
    "    print(f\"전체 레코드: {total_records:,}개\")\n",
    "    print(f\"문화행사가 있는 레코드: {records_with_events:,}개 ({records_with_events/total_records*100:.2f}%)\")\n",
    "    print(f\"총 문화행사 발생: {total_events:,}건\")\n",
    "    \n",
    "    # 16. 분류별 통계\n",
    "    if records_with_events > 0:\n",
    "        print(f\"\\n📈 문화행사 분류별 상위 10개:\")\n",
    "        category_stats = merged_df[merged_df['has_culture_event'] == 1]['분류'].value_counts().head(10)\n",
    "        for category, count in category_stats.items():\n",
    "            print(f\"  {category}: {count}건\")\n",
    "        \n",
    "        print(f\"\\n💰 유무료 통계:\")\n",
    "        fee_stats = merged_df[merged_df['has_culture_event'] == 1]['유무료'].value_counts()\n",
    "        for fee_type, count in fee_stats.items():\n",
    "            print(f\"  {fee_type}: {count}건\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# 실행\n",
    "try:\n",
    "    final_df = merge_culture_with_local_people()\n",
    "    \n",
    "    # 결과 저장\n",
    "    output_file = \"LOCAL_PEOPLE_DONG_with_culture.csv\"\n",
    "    final_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n💾 병합된 데이터가 '{output_file}'로 저장되었습니다.\")\n",
    "    \n",
    "    # 결과 미리보기\n",
    "    print(f\"\\n📋 새로 추가된 문화행사 관련 컬럼:\")\n",
    "    culture_columns = ['has_culture_event', 'culture_event_count', '분류', '유무료']\n",
    "    print(f\"  {culture_columns}\")\n",
    "    \n",
    "    print(f\"\\n📋 최종 데이터 컬럼:\")\n",
    "    print(final_df.columns.tolist())\n",
    "    \n",
    "    print(f\"\\n📋 문화행사가 있는 샘플 데이터:\")\n",
    "    sample_with_culture = final_df[final_df['has_culture_event'] == 1].head(5)\n",
    "    display_cols = ['STDR_DE_ID', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'culture_event_count', '분류', '유무료']\n",
    "    available_cols = [col for col in display_cols if col in final_df.columns]\n",
    "    if len(sample_with_culture) > 0:\n",
    "        print(sample_with_culture[available_cols])\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n",
    "    print(\"다음 파일들이 필요합니다:\")\n",
    "    print(\"1. LOCAL_PEOPLE_DONG_202405_202504_filtered_max.csv\")\n",
    "    print(\"2. 서울시_문화행사_최종완성_수정.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bd56a",
   "metadata": {},
   "source": [
    "#### 공휴일 데이터 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a95b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "수정 전 공휴일 레코드: 192개\n",
      "수정 후 공휴일 레코드: 272개\n",
      "차이: 80개\n",
      "2024-12-25 크리스마스 수정 후: 1\n",
      "수정된 공휴일 정보 병합 중...\n",
      "\n",
      "📊 최종 병합 결과:\n",
      "전체 레코드: 5,840개\n",
      "공휴일 레코드: 272개 (4.66%)\n",
      "\n",
      "💾 수정된 공휴일 정보가 포함된 최종 데이터가 'LOCAL_PEOPLE_DONG_final_corrected.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def merge_holidays_with_corrected_data():\n",
    "    \"\"\"\n",
    "    merged_holidays.csv의 is_holiday를 수정하여 LOCAL_PEOPLE_DONG_with_culture.csv에 병합\n",
    "    \"\"\"\n",
    "    print(\"데이터 로드 중...\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    holidays_df = pd.read_csv(\"merged_holidays.csv\")\n",
    "    culture_df = pd.read_csv(\"LOCAL_PEOPLE_DONG_with_culture.csv\")\n",
    "    \n",
    "    # 2. 2024-2025년 법정공휴일 정의 (누락된 것들 포함)\n",
    "    holidays_2024 = [\n",
    "        20240101,  # 신정\n",
    "        20240209, 20240210, 20240211, 20240212,  # 설날 연휴\n",
    "        20240301,  # 삼일절\n",
    "        20240410,  # 국회의원선거일\n",
    "        20240505, 20240506,  # 어린이날\n",
    "        20240515,  # 부처님오신날\n",
    "        20240606,  # 현충일\n",
    "        20240815,  # 광복절\n",
    "        20240916, 20240917, 20240918,  # 추석 연휴\n",
    "        20241003,  # 개천절\n",
    "        20241009,  # 한글날\n",
    "        20241225,  # 크리스마스 ⭐ 누락된 공휴일\n",
    "    ]\n",
    "    \n",
    "    holidays_2025 = [\n",
    "        20250101,  # 신정\n",
    "        20250128, 20250129, 20250130,  # 설날 연휴\n",
    "        20250301, 20250303,  # 삼일절\n",
    "        20250505, 20250506,  # 어린이날, 부처님오신날\n",
    "        20250606,  # 현충일\n",
    "        20250815,  # 광복절\n",
    "        20251003,  # 개천절\n",
    "        20251006, 20251007, 20251008, 20251009,  # 추석 연휴, 한글날\n",
    "        20251225,  # 크리스마스\n",
    "    ]\n",
    "    \n",
    "    all_holidays = holidays_2024 + holidays_2025\n",
    "    \n",
    "    # 3. 날짜 형식 통일\n",
    "    holidays_df['date_key'] = holidays_df['date'].astype(str)\n",
    "    culture_df['date_key'] = culture_df['STDR_DE_ID'].astype(str)\n",
    "    \n",
    "    # 4. 행정동코드 형식 통일\n",
    "    holidays_df['admin_key'] = holidays_df['ADSTRD_CODE_SE'].astype(str)\n",
    "    culture_df['admin_key'] = culture_df['ADSTRD_CODE_SE'].astype(str)\n",
    "    \n",
    "    # 5. ⭐ 공휴일 정보 수정 (기존 is_holiday 무시하고 새로 계산)\n",
    "    holidays_df['is_holiday_corrected'] = holidays_df['date'].isin(all_holidays).astype(int)\n",
    "    \n",
    "    # 6. 수정 전후 비교\n",
    "    original_holidays = holidays_df['is_holiday'].sum()\n",
    "    corrected_holidays = holidays_df['is_holiday_corrected'].sum()\n",
    "    print(f\"수정 전 공휴일 레코드: {original_holidays:,}개\")\n",
    "    print(f\"수정 후 공휴일 레코드: {corrected_holidays:,}개\")\n",
    "    print(f\"차이: {corrected_holidays - original_holidays:,}개\")\n",
    "    \n",
    "    # 7. 크리스마스 확인\n",
    "    christmas_2024 = holidays_df[holidays_df['date'] == 20241225]['is_holiday_corrected'].iloc[0] if len(holidays_df[holidays_df['date'] == 20241225]) > 0 else \"데이터 없음\"\n",
    "    print(f\"2024-12-25 크리스마스 수정 후: {christmas_2024}\")\n",
    "    \n",
    "    # 8. 수정된 공휴일 정보로 병합\n",
    "    holiday_info = holidays_df[['date_key', 'admin_key', 'is_holiday_corrected']].drop_duplicates()\n",
    "    holiday_info.rename(columns={'is_holiday_corrected': 'is_holiday'}, inplace=True)\n",
    "    \n",
    "    # 9. LOCAL_PEOPLE_DONG_with_culture와 병합\n",
    "    print(\"수정된 공휴일 정보 병합 중...\")\n",
    "    merged_df = culture_df.merge(\n",
    "        holiday_info,\n",
    "        left_on=['date_key', 'admin_key'],\n",
    "        right_on=['date_key', 'admin_key'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 10. 병합되지 않은 경우 기본값 설정\n",
    "    merged_df['is_holiday'] = merged_df['is_holiday'].fillna(0).astype(int)\n",
    "    \n",
    "    # 11. 임시 컬럼 제거\n",
    "    merged_df = merged_df.drop(['date_key', 'admin_key'], axis=1)\n",
    "    \n",
    "    # 12. 결과 통계\n",
    "    total_records = len(merged_df)\n",
    "    holiday_records = len(merged_df[merged_df['is_holiday'] == 1])\n",
    "    \n",
    "    print(f\"\\n📊 최종 병합 결과:\")\n",
    "    print(f\"전체 레코드: {total_records:,}개\")\n",
    "    print(f\"공휴일 레코드: {holiday_records:,}개 ({holiday_records/total_records*100:.2f}%)\")\n",
    "    \n",
    "    # 13. 결과 저장\n",
    "    output_file = \"LOCAL_PEOPLE_DONG_final_corrected.csv\"\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n💾 수정된 공휴일 정보가 포함된 최종 데이터가 '{output_file}'로 저장되었습니다.\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# 실행\n",
    "final_df = merge_holidays_with_corrected_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fadb81",
   "metadata": {},
   "source": [
    "#### 날씨 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c890dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: 'merged_people_weather_utf8sig.csv' (한글 유지 + 깨짐 없음)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 유동인구 데이터 불러오기\n",
    "people_df = pd.read_csv(\"LOCAL_PEOPLE_DONG_final_corrected.csv\")\n",
    "people_df['date'] = pd.to_datetime(people_df['STDR_DE_ID'].astype(str), format=\"%Y%m%d\")\n",
    "\n",
    "# 날씨 데이터 불러오기\n",
    "weather_df = pd.read_csv(\"OBS_ASOS_DD_20250531150815.csv\", encoding=\"cp949\")\n",
    "weather_df['date'] = pd.to_datetime(weather_df['일시'], format=\"%Y-%m-%d\")\n",
    "\n",
    "# 날씨 feature 추출 및 컬럼명 영어로\n",
    "weather_selected = weather_df[[\n",
    "    'date', '평균기온(°C)', '최고기온(°C)', '최저기온(°C)',\n",
    "    '일강수량(mm)', '평균 풍속(m/s)', '합계 일조시간(hr)',\n",
    "    '평균 상대습도(%)', '평균 지면온도(°C)'\n",
    "]].copy()\n",
    "\n",
    "weather_selected.columns = [\n",
    "    'date', 'avg_temp', 'max_temp', 'min_temp',\n",
    "    'precipitation', 'wind_speed', 'sunshine_hours',\n",
    "    'humidity', 'ground_temp'\n",
    "]\n",
    "\n",
    "# 병합\n",
    "merged_df = pd.merge(people_df, weather_selected, on='date', how='left')\n",
    "\n",
    "# ✅ 한글 컬럼/값 보존하면서 저장 (Excel 등에서 깨짐 방지)\n",
    "merged_df.to_csv(\"merged_people_weather_utf8sig.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ 저장 완료: 'merged_people_weather_utf8sig.csv' (한글 유지 + 깨짐 없음)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92064a40",
   "metadata": {},
   "source": [
    "### 지하철 승하차 교통정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "256bd55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 지하철 데이터 구조 수정\n",
      "==================================================\n",
      "원본 데이터: (19110, 6)\n",
      "\n",
      "📋 수정된 데이터 샘플:\n",
      "              사용일자  노선명        역명  승차총승객수  하차총승객수      등록일자\n",
      "20240501  20240501  안산선        안산   10467    9980  20240504\n",
      "20240501  20240501  안산선        초지    4052    4244  20240504\n",
      "20240501  20240501  안산선        고잔    6923    6696  20240504\n",
      "20240501  20240501  안산선        중앙   17412   17710  20240504\n",
      "20240501  20240501  안산선       한대앞    9509    9460  20240504\n",
      "20240501  20240501  안산선       상록수   12528   12649  20240504\n",
      "20240501  20240501  안산선        반월    3871    3879  20240504\n",
      "20240501  20240501  안산선       대야미    4648    4154  20240504\n",
      "20240501  20240501  안산선        산본   14497   15009  20240504\n",
      "20240501  20240501  신림선  관악산(서울대)    4358    4908  20240504\n",
      "\n",
      "📋 날짜 수정 후:\n",
      "              사용일자  노선명   역명  승차총승객수  하차총승객수      등록일자\n",
      "20240501  20240501  안산선   안산   10467    9980  20240504\n",
      "20240501  20240501  안산선   초지    4052    4244  20240504\n",
      "20240501  20240501  안산선   고잔    6923    6696  20240504\n",
      "20240501  20240501  안산선   중앙   17412   17710  20240504\n",
      "20240501  20240501  안산선  한대앞    9509    9460  20240504\n",
      "\n",
      "📋 실제 역명 샘플 (처음 20개):\n",
      "   1. 안산\n",
      "   2. 초지\n",
      "   3. 고잔\n",
      "   4. 중앙\n",
      "   5. 한대앞\n",
      "   6. 상록수\n",
      "   7. 반월\n",
      "   8. 대야미\n",
      "   9. 산본\n",
      "  10. 관악산(서울대)\n",
      "  11. 서울대벤처타운\n",
      "  12. 서원\n",
      "  13. 신림\n",
      "  14. 당곡\n",
      "  15. 보라매병원\n",
      "  16. 보라매공원\n",
      "  17. 보라매\n",
      "  18. 서울지방병무청\n",
      "  19. 대방\n",
      "  20. 샛강\n"
     ]
    }
   ],
   "source": [
    "def fix_subway_data_structure():\n",
    "    \"\"\"\n",
    "    뒤섞인 지하철 데이터 구조를 올바르게 재구성\n",
    "    \"\"\"\n",
    "    print(\"🔧 지하철 데이터 구조 수정\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. 원본 데이터 로드\n",
    "    df = pd.read_csv(\"subway/CARD_SUBWAY_MONTH_202405.csv\", \n",
    "                    encoding='utf-8-sig',\n",
    "                    error_bad_lines=False)\n",
    "    \n",
    "    print(f\"원본 데이터: {df.shape}\")\n",
    "    \n",
    "    # 2. 올바른 컬럼 매핑으로 재구성\n",
    "    df_fixed = pd.DataFrame({\n",
    "        '사용일자': df.index,  # 인덱스가 실제 날짜인 것 같음\n",
    "        '노선명': df['사용일자'],  # 첫 번째 컬럼이 실제 노선명\n",
    "        '역명': df['노선명'],     # 두 번째 컬럼이 실제 역명\n",
    "        '승차총승객수': df['역명'],    # 세 번째 컬럼이 실제 승차승객수\n",
    "        '하차총승객수': df['승차총승객수'], # 네 번째 컬럼이 실제 하차승객수\n",
    "        '등록일자': df['하차총승객수']   # 다섯 번째 컬럼이 실제 등록일자\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n📋 수정된 데이터 샘플:\")\n",
    "    print(df_fixed.head(10))\n",
    "    \n",
    "    # 3. 실제 날짜 추출 (인덱스에서)\n",
    "    # 첫 번째 행의 인덱스가 20240501이므로 이를 활용\n",
    "    first_date = 20240501\n",
    "    df_fixed['사용일자'] = first_date  # 모든 행이 같은 날짜\n",
    "    \n",
    "    print(f\"\\n📋 날짜 수정 후:\")\n",
    "    print(df_fixed.head())\n",
    "    \n",
    "    # 4. 역명 확인\n",
    "    print(f\"\\n📋 실제 역명 샘플 (처음 20개):\")\n",
    "    unique_stations = df_fixed['역명'].unique()\n",
    "    for i, station in enumerate(unique_stations[:20]):\n",
    "        print(f\"  {i+1:2d}. {station}\")\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# 실행\n",
    "fixed_subway_data = fix_subway_data_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7328763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 2025년 2월 파일 인코딩 문제 해결\n",
      "==================================================\n",
      "파일 시작 바이트: b'\\xbb\\xe7\\xbf\\xeb\\xc0\\xcf\\xc0\\xda,\\xb3'\n",
      "❓ BOM 없음, 다양한 인코딩 시도\n",
      "시도 중: utf-8-sig\n",
      "❌ utf-8-sig: 유니코드 오류 - 'utf-8' codec can't decode byte 0xbb in position 0...\n",
      "시도 중: utf-8\n",
      "❌ utf-8: 유니코드 오류 - 'utf-8' codec can't decode byte 0xbb in position 0...\n",
      "시도 중: euc-kr\n",
      "✅ euc-kr 인코딩으로 성공!\n",
      "데이터 크기: (17277, 6)\n",
      "컬럼: ['사용일자', '노선명', '역명', '승차총승객수', '하차총승객수', '등록일자']\n",
      "\n",
      "📋 2월 데이터 샘플:\n",
      "       사용일자  노선명    역명  승차총승객수  하차총승객수      등록일자\n",
      "0  20250201  1호선   서울역   56802   45819  20250204\n",
      "1  20250201  1호선    시청   31681   29724  20250204\n",
      "2  20250201  1호선    종각   27189   25388  20250204\n",
      "3  20250201  1호선  종로3가   25776   23449  20250204\n",
      "4  20250201  1호선  종로5가   20211   20055  20250204\n"
     ]
    }
   ],
   "source": [
    "def fix_202502_encoding_issue():\n",
    "    \"\"\"\n",
    "    2025년 2월 파일 인코딩 문제 완전 해결\n",
    "    \"\"\"\n",
    "    print(\"🔧 2025년 2월 파일 인코딩 문제 해결\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    file_path = \"subway/CARD_SUBWAY_MONTH_202502.csv\"\n",
    "    \n",
    "    # 1. 바이너리 모드로 파일 읽어서 BOM 확인\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_bytes = f.read(10)\n",
    "            print(f\"파일 시작 바이트: {raw_bytes}\")\n",
    "            \n",
    "            # BOM 패턴 확인\n",
    "            if raw_bytes.startswith(b'\\xef\\xbb\\xbf'):\n",
    "                print(\"✅ UTF-8 BOM 감지\")\n",
    "                encoding = 'utf-8-sig'\n",
    "            elif raw_bytes.startswith(b'\\xff\\xfe'):\n",
    "                print(\"✅ UTF-16 LE BOM 감지\")\n",
    "                encoding = 'utf-16'\n",
    "            elif raw_bytes.startswith(b'\\xfe\\xff'):\n",
    "                print(\"✅ UTF-16 BE BOM 감지\")\n",
    "                encoding = 'utf-16'\n",
    "            else:\n",
    "                print(\"❓ BOM 없음, 다양한 인코딩 시도\")\n",
    "                encoding = None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파일 읽기 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. 다양한 인코딩 시도\n",
    "    encodings_to_try = [\n",
    "        'utf-8-sig', 'utf-8', 'euc-kr', 'cp949', 'latin1', \n",
    "        'iso-8859-1', 'utf-16', 'utf-16le', 'utf-16be'\n",
    "    ]\n",
    "    \n",
    "    if encoding:\n",
    "        encodings_to_try.insert(0, encoding)\n",
    "    \n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            print(f\"시도 중: {enc}\")\n",
    "            \n",
    "            # 기본 로드 시도\n",
    "            df = pd.read_csv(file_path, encoding=enc)\n",
    "            print(f\"✅ {enc} 인코딩으로 성공!\")\n",
    "            print(f\"데이터 크기: {df.shape}\")\n",
    "            print(f\"컬럼: {df.columns.tolist()}\")\n",
    "            return df\n",
    "            \n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"❌ {enc}: 유니코드 오류 - {str(e)[:50]}...\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {enc}: 기타 오류 - {str(e)[:50]}...\")\n",
    "            continue\n",
    "    \n",
    "    # 3. 마지막 시도: 오류 무시하고 로드\n",
    "    try:\n",
    "        print(\"\\n🔄 오류 무시 모드로 시도...\")\n",
    "        df = pd.read_csv(file_path, \n",
    "                        encoding='utf-8', \n",
    "                        errors='ignore',\n",
    "                        on_bad_lines='skip')\n",
    "        print(f\"✅ 오류 무시 모드 성공!\")\n",
    "        print(f\"데이터 크기: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 무시 모드도 실패: {e}\")\n",
    "    \n",
    "    # 4. 최후의 수단: 텍스트 에디터로 확인 필요\n",
    "    print(\"\\n💡 해결 방법:\")\n",
    "    print(\"1. 파일을 메모장이나 텍스트 에디터로 열어서 인코딩 확인\")\n",
    "    print(\"2. 다른 인코딩으로 저장 후 재시도\")\n",
    "    print(\"3. 또는 해당 월 데이터를 다시 다운로드\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 실행\n",
    "feb_data = fix_202502_encoding_issue()\n",
    "\n",
    "if feb_data is not None:\n",
    "    print(f\"\\n📋 2월 데이터 샘플:\")\n",
    "    print(feb_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49bc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚇 완전한 12개월 지하철 데이터 처리\n",
      "==================================================\n",
      "✅ subway/CARD_SUBWAY_MONTH_202405.csv: 1519개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202406.csv: 1470개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202407.csv: 1519개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202408.csv: 1519개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202409.csv: 1470개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202410.csv: 1519개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202411.csv: 1470개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202412.csv: 1519개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202501.csv: 1519개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202502.csv: 1372개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202503.csv: 1519개 행 추출\n",
      "✅ subway/CARD_SUBWAY_MONTH_202504.csv: 1422개 행 추출\n",
      "\n",
      "📊 완전한 12개월 추출 결과:\n",
      "총 데이터: 17,837개 행\n",
      "고유 역: 31개\n",
      "고유 행정동: 16개\n",
      "날짜 범위: 20240501 ~ 20250430\n",
      "\n",
      "💾 완전한 12개월 데이터가 'complete_12months_subway_data.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "def process_complete_12_months_subway_data():\n",
    "    \"\"\"\n",
    "    2월 포함 완전한 12개월 지하철 데이터 처리\n",
    "    \"\"\"\n",
    "    print(\"🚇 완전한 12개월 지하철 데이터 처리\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 조사 대상 행정동 지하철역 매핑\n",
    "    station_mapping = {\n",
    "        '종각': '11110615', '종로3가': '11110615', '광화문': '11110615',\n",
    "        '을지로3가': '11110615', '종로5가': '11110615', '동묘앞': '11110650',\n",
    "        '명동': '11140550', '을지로입구': '11140550', '을지로4가': '11140550',\n",
    "        '동대문': '11140605', '동대문역사문화공원': '11140605',\n",
    "        '용산': '11170520', '한강진': '11170520', '삼각지': '11170520',\n",
    "        '이태원': '11170650', '뚝섬': '11200660', '건대입구': '11200660',\n",
    "        '성수': '11200690', '합정': '11440660', '상수': '11440690',\n",
    "        '홍대입구': '11440710', '망원': '11440710', '신촌': '11440710',\n",
    "        '여의도': '11560605', '당산': '11560605', '영등포구청': '11560605',\n",
    "        '신림': '11620745', '봉천': '11620745', '강남': '11680510',\n",
    "        '역삼': '11680640', '선릉': '11680640', '압구정': '11680640',\n",
    "        '잠실': '11710580', '석촌': '11710580'\n",
    "    }\n",
    "    \n",
    "    # 모든 월별 파일 처리 (2월 포함)\n",
    "    subway_files = [\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202405.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202406.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202407.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202408.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202409.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202410.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202411.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202412.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202501.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202502.csv\", 'euc-kr'),  # 2월은 EUC-KR\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202503.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202504.csv\", 'utf-8-sig')\n",
    "    ]\n",
    "    \n",
    "    all_subway_data = []\n",
    "    \n",
    "    for file_path, encoding in subway_files:\n",
    "        try:\n",
    "            # 파일별 적절한 인코딩으로 로드\n",
    "            df = pd.read_csv(file_path, encoding=encoding, error_bad_lines=False)\n",
    "            \n",
    "            # 2월 데이터는 이미 올바른 구조\n",
    "            if '202502' in file_path:\n",
    "                df_processed = df.copy()\n",
    "                # 날짜 컬럼이 이미 올바름\n",
    "            else:\n",
    "                # 다른 월들은 구조 재정렬 필요\n",
    "                df_processed = pd.DataFrame({\n",
    "                    '사용일자': df.index.map(lambda x: int(str(x)[:8]) if len(str(x)) >= 8 else 20240501),\n",
    "                    '노선명': df['사용일자'],\n",
    "                    '역명': df['노선명'],\n",
    "                    '승차총승객수': df['역명'],\n",
    "                    '하차총승객수': df['승차총승객수'],\n",
    "                    '등록일자': df['하차총승객수']\n",
    "                })\n",
    "            \n",
    "            # 조사 대상 역만 필터링\n",
    "            target_stations = list(station_mapping.keys())\n",
    "            df_target = df_processed[df_processed['역명'].isin(target_stations)].copy()\n",
    "            \n",
    "            # 행정동코드 추가\n",
    "            df_target['행정동코드'] = df_target['역명'].map(station_mapping)\n",
    "            \n",
    "            if len(df_target) > 0:\n",
    "                all_subway_data.append(df_target)\n",
    "                print(f\"✅ {file_path}: {len(df_target)}개 행 추출\")\n",
    "            else:\n",
    "                print(f\"❌ {file_path}: 조사 대상 역 없음\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {file_path} 처리 실패: {e}\")\n",
    "    \n",
    "    # 모든 데이터 통합\n",
    "    if all_subway_data:\n",
    "        final_subway_data = pd.concat(all_subway_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n📊 완전한 12개월 추출 결과:\")\n",
    "        print(f\"총 데이터: {len(final_subway_data):,}개 행\")\n",
    "        print(f\"고유 역: {final_subway_data['역명'].nunique()}개\")\n",
    "        print(f\"고유 행정동: {final_subway_data['행정동코드'].nunique()}개\")\n",
    "        print(f\"날짜 범위: {final_subway_data['사용일자'].min()} ~ {final_subway_data['사용일자'].max()}\")\n",
    "        \n",
    "        # 데이터 저장\n",
    "        output_file = \"complete_12months_subway_data.csv\"\n",
    "        final_subway_data.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n💾 완전한 12개월 데이터가 '{output_file}'로 저장되었습니다.\")\n",
    "        \n",
    "        return final_subway_data\n",
    "    else:\n",
    "        print(\"❌ 추출된 데이터가 없습니다.\")\n",
    "        return None\n",
    "\n",
    "# 실행\n",
    "complete_subway_data = process_complete_12_months_subway_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b305542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 지하철 데이터 매핑 품질 체크\n",
      "==================================================\n",
      "📊 전체 데이터 현황:\n",
      "총 데이터: 17,837개 행\n",
      "고유 역: 31개\n",
      "고유 행정동: 16개\n",
      "날짜 범위: 20240501 ~ 20250430\n",
      "\n",
      "📋 행정동별 매핑 현황:\n",
      "  11110615: 4개 역, 365일\n",
      "  11110650: 1개 역, 365일\n",
      "  11140550: 3개 역, 365일\n",
      "  11140605: 1개 역, 365일\n",
      "  11170520: 3개 역, 365일\n",
      "  11170650: 1개 역, 365일\n",
      "  11200660: 2개 역, 365일\n",
      "  11200690: 1개 역, 365일\n",
      "  11440660: 1개 역, 365일\n",
      "  11440690: 1개 역, 365일\n",
      "  11440710: 3개 역, 365일\n",
      "  11560605: 3개 역, 365일\n",
      "  11620745: 2개 역, 365일\n",
      "  11680510: 1개 역, 365일\n",
      "  11680640: 3개 역, 365일\n",
      "  11710580: 1개 역, 365일\n",
      "\n",
      "📋 역별 데이터 분포:\n",
      "  홍대입구 (11440710): 1095개 행\n",
      "  종로3가 (11110615): 1095개 행\n",
      "  동묘앞 (11110650): 730개 행\n",
      "  여의도 (11560605): 730개 행\n",
      "  당산 (11560605): 730개 행\n",
      "  을지로3가 (11110615): 730개 행\n",
      "  을지로4가 (11140550): 730개 행\n",
      "  신촌 (11440710): 730개 행\n",
      "  동대문 (11140605): 730개 행\n",
      "  건대입구 (11200660): 730개 행\n",
      "  선릉 (11680640): 730개 행\n",
      "  합정 (11440660): 730개 행\n",
      "  영등포구청 (11560605): 730개 행\n",
      "  석촌 (11710580): 730개 행\n",
      "  신림 (11620745): 730개 행\n",
      "  삼각지 (11170520): 682개 행\n",
      "  이태원 (11170650): 365개 행\n",
      "  강남 (11680510): 365개 행\n",
      "  용산 (11170520): 365개 행\n",
      "  한강진 (11170520): 365개 행\n",
      "  을지로입구 (11140550): 365개 행\n",
      "  상수 (11440690): 365개 행\n",
      "  종각 (11110615): 365개 행\n",
      "  망원 (11440710): 365개 행\n",
      "  역삼 (11680640): 365개 행\n",
      "  뚝섬 (11200660): 365개 행\n",
      "  압구정 (11680640): 365개 행\n",
      "  성수 (11200690): 365개 행\n",
      "  명동 (11140550): 365개 행\n",
      "  종로5가 (11110615): 365개 행\n",
      "  봉천 (11620745): 365개 행\n",
      "\n",
      "📋 월별 데이터 분포:\n",
      "  01월: 1519개 행\n",
      "  02월: 1372개 행\n",
      "  03월: 1519개 행\n",
      "  04월: 1422개 행\n",
      "  05월: 1519개 행\n",
      "  06월: 1470개 행\n",
      "  07월: 1519개 행\n",
      "  08월: 1519개 행\n",
      "  09월: 1470개 행\n",
      "  10월: 1519개 행\n",
      "  11월: 1470개 행\n",
      "  12월: 1519개 행\n",
      "\n",
      "🔍 데이터 품질 체크:\n",
      "결측치:\n",
      "  역명: 0개\n",
      "  행정동코드: 0개\n",
      "  하차승객수: 0개\n",
      "\n",
      "승객수 통계:\n",
      "  평균 하차승객수: 23459.7명\n",
      "  최대 하차승객수: 116,745명\n",
      "  최소 하차승객수: 122명\n",
      "\n",
      "📋 샘플 데이터 (각 행정동별 1개씩):\n",
      "             역명      사용일자  하차총승객수\n",
      "행정동코드                            \n",
      "11110615   종로5가  20240501   24236\n",
      "11110650    동묘앞  20240501   13926\n",
      "11140550  을지로입구  20240501   33688\n",
      "11140605    동대문  20240501   13214\n",
      "11170520    삼각지  20240501    6555\n",
      "11170650    이태원  20240501   11796\n",
      "11200660     뚝섬  20240501   19878\n",
      "11200690     성수  20240501   38758\n",
      "11440660     합정  20240501   29691\n",
      "11440690     상수  20240501   11108\n",
      "11440710   홍대입구  20240501   15051\n",
      "11560605  영등포구청  20240501   14659\n",
      "11620745     신림  20240501    2302\n",
      "11680510     강남  20240501   50462\n",
      "11680640     선릉  20240501    8778\n",
      "11710580     석촌  20240501    8405\n"
     ]
    }
   ],
   "source": [
    "def check_subway_mapping_quality():\n",
    "    \"\"\"\n",
    "    지하철 데이터 매핑 품질 체크\n",
    "    \"\"\"\n",
    "    print(\"🔍 지하철 데이터 매핑 품질 체크\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 완전한 12개월 데이터 로드\n",
    "    subway_df = pd.read_csv(\"complete_12months_subway_data.csv\")\n",
    "    \n",
    "    print(f\"📊 전체 데이터 현황:\")\n",
    "    print(f\"총 데이터: {len(subway_df):,}개 행\")\n",
    "    print(f\"고유 역: {subway_df['역명'].nunique()}개\")\n",
    "    print(f\"고유 행정동: {subway_df['행정동코드'].nunique()}개\")\n",
    "    print(f\"날짜 범위: {subway_df['사용일자'].min()} ~ {subway_df['사용일자'].max()}\")\n",
    "    \n",
    "    # 1. 행정동별 매핑 현황\n",
    "    print(f\"\\n📋 행정동별 매핑 현황:\")\n",
    "    admin_mapping = subway_df.groupby('행정동코드').agg({\n",
    "        '역명': 'nunique',\n",
    "        '사용일자': 'nunique'\n",
    "    }).rename(columns={'역명': '역수', '사용일자': '일수'})\n",
    "    \n",
    "    for admin_code, row in admin_mapping.iterrows():\n",
    "        print(f\"  {admin_code}: {row['역수']}개 역, {row['일수']}일\")\n",
    "    \n",
    "    # 2. 역별 데이터 분포\n",
    "    print(f\"\\n📋 역별 데이터 분포:\")\n",
    "    station_counts = subway_df['역명'].value_counts()\n",
    "    for station, count in station_counts.items():\n",
    "        admin_code = subway_df[subway_df['역명'] == station]['행정동코드'].iloc[0]\n",
    "        print(f\"  {station} ({admin_code}): {count}개 행\")\n",
    "    \n",
    "    # 3. 월별 데이터 분포\n",
    "    print(f\"\\n📋 월별 데이터 분포:\")\n",
    "    subway_df['월'] = subway_df['사용일자'].astype(str).str[4:6]\n",
    "    monthly_counts = subway_df['월'].value_counts().sort_index()\n",
    "    for month, count in monthly_counts.items():\n",
    "        print(f\"  {month}월: {count}개 행\")\n",
    "    \n",
    "    # 4. 데이터 품질 체크\n",
    "    print(f\"\\n🔍 데이터 품질 체크:\")\n",
    "    \n",
    "    # 결측치 확인\n",
    "    missing_station = subway_df['역명'].isnull().sum()\n",
    "    missing_admin = subway_df['행정동코드'].isnull().sum()\n",
    "    missing_inflow = subway_df['하차총승객수'].isnull().sum()\n",
    "    \n",
    "    print(f\"결측치:\")\n",
    "    print(f\"  역명: {missing_station}개\")\n",
    "    print(f\"  행정동코드: {missing_admin}개\")\n",
    "    print(f\"  하차승객수: {missing_inflow}개\")\n",
    "    \n",
    "    # 이상값 확인\n",
    "    print(f\"\\n승객수 통계:\")\n",
    "    print(f\"  평균 하차승객수: {subway_df['하차총승객수'].mean():.1f}명\")\n",
    "    print(f\"  최대 하차승객수: {subway_df['하차총승객수'].max():,}명\")\n",
    "    print(f\"  최소 하차승객수: {subway_df['하차총승객수'].min():,}명\")\n",
    "    \n",
    "    # 5. 샘플 데이터 확인\n",
    "    print(f\"\\n📋 샘플 데이터 (각 행정동별 1개씩):\")\n",
    "    sample_data = subway_df.groupby('행정동코드').first()\n",
    "    print(sample_data[['역명', '사용일자', '하차총승객수']])\n",
    "    \n",
    "    return subway_df\n",
    "\n",
    "# 실행\n",
    "mapping_check = check_subway_mapping_quality()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7b9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 최종 데이터셋 생성 시작!\n",
      "🎯 최종 데이터셋 생성 (승차/하차 데이터 포함)\n",
      "==================================================\n",
      "1단계: 메인 데이터셋 로드\n",
      "메인 데이터셋: 5,840개 행\n",
      "컬럼: ['STDR_DE_ID', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'culture_event_count', '분류', '유무료', 'has_culture_event', 'is_holiday', 'avg_temp', 'max_temp', 'min_temp', 'precipitation', 'wind_speed', 'sunshine_hours', 'humidity', 'ground_temp']\n",
      "\n",
      "2단계: 지하철 데이터 로드\n",
      "지하철 데이터: 17,837개 행\n",
      "고유 역: 31개\n",
      "고유 행정동: 16개\n",
      "\n",
      "3단계: 지하철 데이터 집계\n",
      "집계된 지하철 데이터: 5,840개 행\n",
      "고유 날짜: 365개\n",
      "고유 행정동: 16개\n",
      "\n",
      "4단계: 병합 키 생성\n",
      "메인 데이터 키 샘플:    date_key admin_key\n",
      "0  20240501  11110615\n",
      "1  20240501  11110650\n",
      "2  20240501  11140550\n",
      "3  20240501  11140605\n",
      "4  20240501  11170520\n",
      "지하철 데이터 키 샘플:    date_key admin_key\n",
      "0  20240501  11110615\n",
      "1  20240501  11110650\n",
      "2  20240501  11140550\n",
      "3  20240501  11140605\n",
      "4  20240501  11170520\n",
      "\n",
      "5단계: 데이터 병합\n",
      "병합 후 데이터: 5,840개 행\n",
      "\n",
      "6단계: 결측치 처리\n",
      "유입 결측치 처리: 0개 → 0개\n",
      "유출 결측치 처리: 0개 → 0개\n",
      "\n",
      "7단계: 결과 확인\n",
      "📊 최종 병합 결과:\n",
      "전체 레코드: 5,840개\n",
      "지하철 데이터가 있는 레코드: 5,840개\n",
      "지하철 데이터 비율: 100.00%\n",
      "평균 지하철 유입객: 71652.6명\n",
      "평균 지하철 유출객: 69750.5명\n",
      "최대 지하철 유입객: 237,061명\n",
      "최대 지하철 유출객: 206,455명\n",
      "\n",
      "📋 행정동별 지하철 이용 현황:\n",
      "                    평균유입    최대유입      평균유출    최대유출\n",
      "ADSTRD_CODE_SE                                    \n",
      "11110615        143575.4  191291  146001.4  206455\n",
      "11110650         19394.4   29385   19099.2   29751\n",
      "11140550        104204.4  147915  100980.7  141497\n",
      "11140605         30993.4   39806   30738.3   42071\n",
      "11170520         69034.0  105275   66710.3  103345\n",
      "11170650         12801.4   22120   11635.2   22170\n",
      "11200660         76370.9   99537   73380.7   95787\n",
      "11200690         47572.9   65447   43613.3   61084\n",
      "11440660         47854.9   64957   45254.4   57617\n",
      "11440690         11454.6   15775   10189.5   13483\n",
      "11440710        151613.7  207820  144094.6  190800\n",
      "11560605        119152.2  237061  115947.1  187716\n",
      "11620745         75854.4   92389   78414.5   96380\n",
      "11680510         74030.3   99530   75321.4  100409\n",
      "11680640        144573.5  194317  136996.0  185736\n",
      "11710580         17961.4   30376   17631.6   26993\n",
      "\n",
      "8단계: 데이터 저장\n",
      "💾 최종 데이터셋이 'final_dataset_with_subway.csv'로 저장되었습니다.\n",
      "\n",
      "📋 최종 데이터셋 컬럼 (18개):\n",
      "   1. STDR_DE_ID\n",
      "   2. ADSTRD_CODE_SE\n",
      "   3. TOT_LVPOP_CO\n",
      "   4. culture_event_count\n",
      "   5. 분류\n",
      "   6. 유무료\n",
      "   7. has_culture_event\n",
      "   8. is_holiday\n",
      "   9. avg_temp\n",
      "  10. max_temp\n",
      "  11. min_temp\n",
      "  12. precipitation\n",
      "  13. wind_speed\n",
      "  14. sunshine_hours\n",
      "  15. humidity\n",
      "  16. ground_temp\n",
      "  17. subway_inflow\n",
      "  18. subway_outflow\n",
      "\n",
      "📋 최종 데이터 샘플:\n",
      "   STDR_DE_ID  ADSTRD_CODE_SE  TOT_LVPOP_CO  subway_inflow  subway_outflow  \\\n",
      "0    20240501        11110615    99785.2975         123392          130977   \n",
      "1    20240501        11110650    43377.5306          23113           22273   \n",
      "2    20240501        11140550    54393.7284          87651           88445   \n",
      "3    20240501        11140605    20673.6264          37771           36987   \n",
      "4    20240501        11170520    13714.7677          63437           60673   \n",
      "5    20240501        11170650    13215.9398          11796           12025   \n",
      "6    20240501        11200660    24900.6383          71512           71515   \n",
      "7    20240501        11200690    40082.2197          38758           34811   \n",
      "8    20240501        11440660   106161.9538          41878           40872   \n",
      "9    20240501        11440690    23856.3010          11108           10315   \n",
      "\n",
      "   is_holiday  has_culture_event  \n",
      "0           0                  0  \n",
      "1           0                  0  \n",
      "2           0                  0  \n",
      "3           0                  0  \n",
      "4           0                  0  \n",
      "5           0                  0  \n",
      "6           0                  0  \n",
      "7           0                  0  \n",
      "8           0                  0  \n",
      "9           0                  0  \n",
      "\n",
      "✅ 모든 처리가 완료되었습니다!\n",
      "최종 데이터셋 크기: (5840, 18)\n",
      "📈 생활인구 예측을 위한 완전한 데이터셋이 준비되었습니다!\n",
      "\n",
      "🎯 새로 추가된 feature:\n",
      "  - subway_inflow: 지하철 유입 승객수 (하차)\n",
      "  - subway_outflow: 지하철 유출 승객수 (승차)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_final_dataset_with_subway():\n",
    "    \"\"\"\n",
    "    승차/하차 승객수로 최종 데이터셋 생성\n",
    "    \"\"\"\n",
    "    print(\"🎯 최종 데이터셋 생성 (승차/하차 데이터 포함)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. 메인 데이터셋 로드\n",
    "    print(\"1단계: 메인 데이터셋 로드\")\n",
    "    main_df = pd.read_csv(\"dataset.csv\")\n",
    "    print(f\"메인 데이터셋: {len(main_df):,}개 행\")\n",
    "    print(f\"컬럼: {main_df.columns.tolist()}\")\n",
    "    \n",
    "    # 2. 완전한 12개월 지하철 데이터 로드\n",
    "    print(\"\\n2단계: 지하철 데이터 로드\")\n",
    "    subway_df = pd.read_csv(\"complete_12months_subway_data.csv\")\n",
    "    print(f\"지하철 데이터: {len(subway_df):,}개 행\")\n",
    "    print(f\"고유 역: {subway_df['역명'].nunique()}개\")\n",
    "    print(f\"고유 행정동: {subway_df['행정동코드'].nunique()}개\")\n",
    "    \n",
    "    # 3. 지하철 데이터를 행정동별, 날짜별로 집계 (승차/하차 모두)\n",
    "    print(\"\\n3단계: 지하철 데이터 집계\")\n",
    "    subway_agg = subway_df.groupby(['사용일자', '행정동코드']).agg({\n",
    "        '하차총승객수': 'sum',  # 유입 (해당 지역으로 들어오는 사람)\n",
    "        '승차총승객수': 'sum'   # 유출 (해당 지역에서 나가는 사람)\n",
    "    }).reset_index()\n",
    "    \n",
    "    subway_agg.rename(columns={\n",
    "        '하차총승객수': 'subway_inflow',   # 유입\n",
    "        '승차총승객수': 'subway_outflow'   # 유출\n",
    "    }, inplace=True)\n",
    "    \n",
    "    print(f\"집계된 지하철 데이터: {len(subway_agg):,}개 행\")\n",
    "    print(f\"고유 날짜: {subway_agg['사용일자'].nunique()}개\")\n",
    "    print(f\"고유 행정동: {subway_agg['행정동코드'].nunique()}개\")\n",
    "    \n",
    "    # 4. 병합 키 생성\n",
    "    print(\"\\n4단계: 병합 키 생성\")\n",
    "    main_df['date_key'] = main_df['STDR_DE_ID'].astype(str)\n",
    "    main_df['admin_key'] = main_df['ADSTRD_CODE_SE'].astype(str)\n",
    "    subway_agg['date_key'] = subway_agg['사용일자'].astype(str)\n",
    "    subway_agg['admin_key'] = subway_agg['행정동코드'].astype(str)\n",
    "    \n",
    "    print(f\"메인 데이터 키 샘플: {main_df[['date_key', 'admin_key']].head()}\")\n",
    "    print(f\"지하철 데이터 키 샘플: {subway_agg[['date_key', 'admin_key']].head()}\")\n",
    "    \n",
    "    # 5. Left Join 실행\n",
    "    print(\"\\n5단계: 데이터 병합\")\n",
    "    merged_df = main_df.merge(\n",
    "        subway_agg[['date_key', 'admin_key', 'subway_inflow', 'subway_outflow']],\n",
    "        on=['date_key', 'admin_key'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"병합 후 데이터: {len(merged_df):,}개 행\")\n",
    "    \n",
    "    # 6. 결측치 처리\n",
    "    print(\"\\n6단계: 결측치 처리\")\n",
    "    before_fill_inflow = merged_df['subway_inflow'].isnull().sum()\n",
    "    before_fill_outflow = merged_df['subway_outflow'].isnull().sum()\n",
    "    \n",
    "    merged_df['subway_inflow'] = merged_df['subway_inflow'].fillna(0).astype(int)\n",
    "    merged_df['subway_outflow'] = merged_df['subway_outflow'].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"유입 결측치 처리: {before_fill_inflow}개 → 0개\")\n",
    "    print(f\"유출 결측치 처리: {before_fill_outflow}개 → 0개\")\n",
    "    \n",
    "    # 7. 임시 컬럼 제거\n",
    "    merged_df = merged_df.drop(['date_key', 'admin_key'], axis=1)\n",
    "    \n",
    "    # 8. 결과 확인\n",
    "    print(\"\\n7단계: 결과 확인\")\n",
    "    total_records = len(merged_df)\n",
    "    records_with_subway = len(merged_df[merged_df['subway_inflow'] > 0])\n",
    "    \n",
    "    print(f\"📊 최종 병합 결과:\")\n",
    "    print(f\"전체 레코드: {total_records:,}개\")\n",
    "    print(f\"지하철 데이터가 있는 레코드: {records_with_subway:,}개\")\n",
    "    print(f\"지하철 데이터 비율: {records_with_subway/total_records*100:.2f}%\")\n",
    "    print(f\"평균 지하철 유입객: {merged_df['subway_inflow'].mean():.1f}명\")\n",
    "    print(f\"평균 지하철 유출객: {merged_df['subway_outflow'].mean():.1f}명\")\n",
    "    print(f\"최대 지하철 유입객: {merged_df['subway_inflow'].max():,}명\")\n",
    "    print(f\"최대 지하철 유출객: {merged_df['subway_outflow'].max():,}명\")\n",
    "    \n",
    "    # 9. 행정동별 지하철 이용 현황\n",
    "    print(f\"\\n📋 행정동별 지하철 이용 현황:\")\n",
    "    admin_stats = merged_df.groupby('ADSTRD_CODE_SE').agg({\n",
    "        'subway_inflow': ['mean', 'max'],\n",
    "        'subway_outflow': ['mean', 'max']\n",
    "    }).round(1)\n",
    "    admin_stats.columns = ['평균유입', '최대유입', '평균유출', '최대유출']\n",
    "    print(admin_stats)\n",
    "    \n",
    "    # 10. 최종 데이터 저장\n",
    "    print(\"\\n8단계: 데이터 저장\")\n",
    "    output_file = \"final_dataset_with_subway.csv\"\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"💾 최종 데이터셋이 '{output_file}'로 저장되었습니다.\")\n",
    "    \n",
    "    # 11. 최종 컬럼 확인\n",
    "    print(f\"\\n📋 최종 데이터셋 컬럼 ({len(merged_df.columns)}개):\")\n",
    "    for i, col in enumerate(merged_df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    # 12. 샘플 데이터\n",
    "    print(f\"\\n📋 최종 데이터 샘플:\")\n",
    "    sample_cols = ['STDR_DE_ID', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'subway_inflow', 'subway_outflow', 'is_holiday', 'has_culture_event']\n",
    "    print(merged_df[sample_cols].head(10))\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# 실행\n",
    "print(\"🚀 최종 데이터셋 생성 시작!\")\n",
    "final_dataset = create_final_dataset_with_subway()\n",
    "\n",
    "if final_dataset is not None:\n",
    "    print(\"\\n✅ 모든 처리가 완료되었습니다!\")\n",
    "    print(f\"최종 데이터셋 크기: {final_dataset.shape}\")\n",
    "    print(\"📈 생활인구 예측을 위한 완전한 데이터셋이 준비되었습니다!\")\n",
    "    print(\"\\n🎯 새로 추가된 feature:\")\n",
    "    print(\"  - subway_inflow: 지하철 유입 승객수 (하차)\")\n",
    "    print(\"  - subway_outflow: 지하철 유출 승객수 (승차)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62283cfa",
   "metadata": {},
   "source": [
    "#### 버스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8c1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 7자리 코드 매칭 시도\n",
      "==================================================\n",
      "조사 대상 8자리 코드: 16개\n",
      "조사 대상 7자리 코드: 16개\n",
      "\n",
      "📊 7자리 매칭 결과:\n",
      "매칭된 데이터: 8,701개 행\n",
      "매칭된 고유 7자리 코드: 7개\n",
      "\n",
      "✅ 매칭된 7자리 코드 (7개):\n",
      "  1111061 → ['11110615']\n",
      "  1111065 → ['11110650']\n",
      "  1114060 → ['11140605']\n",
      "  1117052 → ['11170520']\n",
      "  1117065 → ['11170650']\n",
      "  1120066 → ['11200660']\n",
      "  1120069 → ['11200690']\n",
      "\n",
      "❌ 매칭 안된 7자리 코드 (9개):\n",
      "  1114055 → ['11140550']\n",
      "  1144066 → ['11440660']\n",
      "  1144069 → ['11440690']\n",
      "  1144071 → ['11440710']\n",
      "  1156060 → ['11560605']\n",
      "  1162074 → ['11620745']\n",
      "  1168051 → ['11680510']\n",
      "  1168064 → ['11680640']\n",
      "  1171058 → ['11710580']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89086074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️ 확장된 매핑 테이블 생성\n",
      "==================================================\n",
      "🔍 매칭 안된 행정동 심층 분석\n",
      "==================================================\n",
      "📋 버스 데이터의 모든 고유 행정동 코드:\n",
      "\n",
      "🔍 1114055 관련 코드 찾기:\n",
      "  📋 6자리 유사: ['1114059', '11140590']...\n",
      "  📋 5자리 유사: ['1114059', '11140590', '1114060', '11140600', '1114061']...\n",
      "\n",
      "🔍 1144066 관련 코드 찾기:\n",
      "\n",
      "🔍 1144069 관련 코드 찾기:\n",
      "\n",
      "🔍 1144071 관련 코드 찾기:\n",
      "\n",
      "🔍 1156060 관련 코드 찾기:\n",
      "\n",
      "🔍 1162074 관련 코드 찾기:\n",
      "\n",
      "🔍 1168051 관련 코드 찾기:\n",
      "\n",
      "🔍 1168064 관련 코드 찾기:\n",
      "\n",
      "🔍 1171058 관련 코드 찾기:\n",
      "\n",
      "🔍 특별 패턴 분석:\n",
      "강남구(1168) 관련 코드: []\n",
      "마포구(1144) 관련 코드: []\n",
      "영등포구(1156) 관련 코드: []\n",
      "기존 매핑: 7개\n",
      "확장 후 매핑: 7개\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9f619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
