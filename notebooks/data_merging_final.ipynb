{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242e0e0a",
   "metadata": {},
   "source": [
    "### ë¬¸í™” í–‰ì‚¬ ì •ë³´ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e99c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5597 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ 'cp949' ì¸ì½”ë”© ì‹œë„ ì¤‘...\n",
      "âŒ ì‹¤íŒ¨ (cp949): 'cp949' codec can't decode byte 0x98 in position 8: illegal multibyte sequence\n",
      "â³ 'utf-8' ì¸ì½”ë”© ì‹œë„ ì¤‘...\n",
      "âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ: utf-8\n",
      "ì´ 5597ê°œì˜ í–‰ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "ì‚¬ìš©í•  ìœ„ë„: ìœ„ë„(Yì¢Œí‘œ), ê²½ë„: ê²½ë„(Xì¢Œí‘œ)\n",
      "ì‚¬ìš©í•  ë‚ ì§œ: ë‚ ì§œ/ì‹œê°„, ì‹œê°„: ë‚ ì§œ/ì‹œê°„\n",
      "ğŸ—ºï¸ ì¢Œí‘œ â†’ í–‰ì •ë™ ì •ë³´ ë³€í™˜ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5597/5597 [16:47<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ìµœì¢… CSV ì €ì¥ ì™„ë£Œ: ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì¹´ì¹´ì˜¤ REST API í‚¤ ì…ë ¥ (https://developers.kakao.com)\n",
    "KAKAO_API_KEY = \"api_key\"  # ì—¬ê¸°ì— ë³¸ì¸ì˜ REST API í‚¤ ì…ë ¥\n",
    "\n",
    "CSV_FILE_PATH = \"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv\"  # ì›ë³¸ CSV ê²½ë¡œ\n",
    "\n",
    "# 1. í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ robust CSV ì½ê¸° í•¨ìˆ˜\n",
    "def read_csv_robust(file_path):\n",
    "    encodings = ['cp949', 'utf-8', 'euc-kr', 'utf-8-sig']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"â³ '{encoding}' ì¸ì½”ë”© ì‹œë„ ì¤‘...\")\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ: {encoding}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì‹¤íŒ¨ ({encoding}): {e}\")\n",
    "    raise ValueError(\"âš ï¸ ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨\")\n",
    "\n",
    "# 2. ì¹´ì¹´ì˜¤ reverse geocoding í•¨ìˆ˜\n",
    "def get_admin_code_kakao(lat, lng, api_key):\n",
    "    url = \"https://dapi.kakao.com/v2/local/geo/coord2regioncode.json\"\n",
    "    headers = {\"Authorization\": f\"KakaoAK {api_key}\"}\n",
    "    params = {\"x\": lng, \"y\": lat}\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, params=params, timeout=5)\n",
    "        res.raise_for_status()\n",
    "        data = res.json()\n",
    "        for doc in data.get(\"documents\", []):\n",
    "            if doc['region_type'] == 'H':  # í–‰ì •ë™ ìš°ì„ \n",
    "                return {\n",
    "                    'í–‰ì •ë™ì½”ë“œ': doc['code'],\n",
    "                    'ì‹œë„': doc['region_1depth_name'],\n",
    "                    'ì‹œêµ°êµ¬': doc['region_2depth_name'],\n",
    "                    'í–‰ì •ë™': doc['region_3depth_name'],\n",
    "                    'ì„±ê³µ': True,\n",
    "                    'ì˜¤ë¥˜': None\n",
    "                }\n",
    "        return {'í–‰ì •ë™ì½”ë“œ': None, 'ì‹œë„': None, 'ì‹œêµ°êµ¬': None, 'í–‰ì •ë™': None, 'ì„±ê³µ': False, 'ì˜¤ë¥˜': 'No region_type H'}\n",
    "    except Exception as e:\n",
    "        return {'í–‰ì •ë™ì½”ë“œ': None, 'ì‹œë„': None, 'ì‹œêµ°êµ¬': None, 'í–‰ì •ë™': None, 'ì„±ê³µ': False, 'ì˜¤ë¥˜': str(e)}\n",
    "\n",
    "# 3. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def main():\n",
    "    df = read_csv_robust(CSV_FILE_PATH)\n",
    "    print(f\"ì´ {len(df)}ê°œì˜ í–‰ì´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ìœ„ë„/ê²½ë„ ì»¬ëŸ¼ ìë™ íƒì§€\n",
    "    lat_col = next((col for col in df.columns if \"ìœ„ë„\" in col or \"Y\" in col or \"lat\" in col.lower()), None)\n",
    "    lng_col = next((col for col in df.columns if \"ê²½ë„\" in col or \"X\" in col or \"lon\" in col.lower()), None)\n",
    "    if lat_col is None or lng_col is None:\n",
    "        raise ValueError(\"âš ï¸ ìœ„ë„/ê²½ë„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ ìë™ íƒì§€\n",
    "    date_col = next((col for col in df.columns if \"ì¼\" in col or \"ë‚ ì§œ\" in col), None)\n",
    "    time_col = next((col for col in df.columns if \"ì‹œ\" in col or \"ì‹œê°„\" in col), None)\n",
    "\n",
    "    print(f\"ì‚¬ìš©í•  ìœ„ë„: {lat_col}, ê²½ë„: {lng_col}\")\n",
    "    print(f\"ì‚¬ìš©í•  ë‚ ì§œ: {date_col}, ì‹œê°„: {time_col}\")\n",
    "\n",
    "    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    í–‰ì •ë™ì½”ë“œ, ì‹œë„, ì‹œêµ°êµ¬, í–‰ì •ë™, ì„±ê³µ, ì˜¤ë¥˜ = [], [], [], [], [], []\n",
    "\n",
    "    print(\"ğŸ—ºï¸ ì¢Œí‘œ â†’ í–‰ì •ë™ ì •ë³´ ë³€í™˜ ì¤‘...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        lat, lng = row[lat_col], row[lng_col]\n",
    "        if pd.isna(lat) or pd.isna(lng):\n",
    "            í–‰ì •ë™ì½”ë“œ.append(None); ì‹œë„.append(None); ì‹œêµ°êµ¬.append(None); í–‰ì •ë™.append(None)\n",
    "            ì„±ê³µ.append(False); ì˜¤ë¥˜.append(\"Invalid coord\")\n",
    "            continue\n",
    "        result = get_admin_code_kakao(lat, lng, KAKAO_API_KEY)\n",
    "        í–‰ì •ë™ì½”ë“œ.append(result['í–‰ì •ë™ì½”ë“œ'])\n",
    "        ì‹œë„.append(result['ì‹œë„'])\n",
    "        ì‹œêµ°êµ¬.append(result['ì‹œêµ°êµ¬'])\n",
    "        í–‰ì •ë™.append(result['í–‰ì •ë™'])\n",
    "        ì„±ê³µ.append(result['ì„±ê³µ'])\n",
    "        ì˜¤ë¥˜.append(result['ì˜¤ë¥˜'])\n",
    "        time.sleep(0.1)  # ì¹´ì¹´ì˜¤ ì´ˆë‹¹ 10íšŒ ì œí•œ\n",
    "\n",
    "    # ê²°ê³¼ ê²°í•©\n",
    "    df[\"í–‰ì •ë™ì½”ë“œ\"] = í–‰ì •ë™ì½”ë“œ\n",
    "    df[\"ì‹œë„\"] = ì‹œë„\n",
    "    df[\"ì‹œêµ°êµ¬\"] = ì‹œêµ°êµ¬\n",
    "    df[\"í–‰ì •ë™\"] = í–‰ì •ë™\n",
    "    df[\"geocoding_success\"] = ì„±ê³µ\n",
    "    df[\"geocoding_error\"] = ì˜¤ë¥˜\n",
    "\n",
    "    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì €ì¥\n",
    "    save_cols = [col for col in [date_col, time_col, lat_col, lng_col, \"í–‰ì •ë™ì½”ë“œ\", \"ì‹œë„\", \"ì‹œêµ°êµ¬\", \"í–‰ì •ë™\"] if col]\n",
    "    df_result = df[save_cols]\n",
    "\n",
    "    output_file = \"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv\"\n",
    "    df_result.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nâœ… ìµœì¢… CSV ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025fda78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë“œëœ ë°ì´í„°: 5597ê°œ\n",
      "ì»¬ëŸ¼: ['ë‚ ì§œ/ì‹œê°„', 'ë‚ ì§œ/ì‹œê°„.1', 'ìœ„ë„(Yì¢Œí‘œ)', 'ê²½ë„(Xì¢Œí‘œ)', 'í–‰ì •ë™ì½”ë“œ', 'ì‹œë„', 'ì‹œêµ°êµ¬', 'í–‰ì •ë™']\n",
      "                   ë‚ ì§œ/ì‹œê°„                ë‚ ì§œ/ì‹œê°„.1    ìœ„ë„(Yì¢Œí‘œ)     ê²½ë„(Xì¢Œí‘œ)  \\\n",
      "0  2025-12-18~2025-12-21  2025-12-18~2025-12-21  37.511824  127.059159   \n",
      "1  2025-12-06~2025-12-06  2025-12-06~2025-12-06  37.549906  126.945534   \n",
      "2  2025-10-18~2025-10-19  2025-10-18~2025-10-19  37.457066  126.896037   \n",
      "3  2025-10-03~2025-10-12  2025-10-03~2025-10-12  37.529365  127.073978   \n",
      "4  2025-09-27~2025-09-28  2025-09-27~2025-09-28  37.641994  127.077437   \n",
      "\n",
      "          í–‰ì •ë™ì½”ë“œ     ì‹œë„  ì‹œêµ°êµ¬   í–‰ì •ë™  \n",
      "0  1.168058e+09  ì„œìš¸íŠ¹ë³„ì‹œ  ê°•ë‚¨êµ¬  ì‚¼ì„±1ë™  \n",
      "1  1.144060e+09  ì„œìš¸íŠ¹ë³„ì‹œ  ë§ˆí¬êµ¬   ëŒ€í¥ë™  \n",
      "2  1.154567e+09  ì„œìš¸íŠ¹ë³„ì‹œ  ê¸ˆì²œêµ¬  ì‹œí¥1ë™  \n",
      "3  1.121584e+09  ì„œìš¸íŠ¹ë³„ì‹œ  ê´‘ì§„êµ¬  ìì–‘3ë™  \n",
      "4  1.135061e+09  ì„œìš¸íŠ¹ë³„ì‹œ  ë…¸ì›êµ¬  í•˜ê³„1ë™  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ê°€ì¥ ì™„ì„±ëœ ë¬¸í™”í–‰ì‚¬ ë°ì´í„° ë¡œë“œ\n",
    "culture_df = pd.read_csv(\"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv\", encoding='utf-8-sig')\n",
    "print(f\"ë¡œë“œëœ ë°ì´í„°: {len(culture_df)}ê°œ\")\n",
    "print(f\"ì»¬ëŸ¼: {culture_df.columns.tolist()}\")\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(culture_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09af43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì§€ëœ ì¸ì½”ë”©: {'encoding': 'UTF-8-SIG', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# íŒŒì¼ì˜ ì‹¤ì œ ì¸ì½”ë”© í™•ì¸\n",
    "def detect_file_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result\n",
    "\n",
    "# ì¸ì½”ë”© í™•ì¸\n",
    "encoding_info = detect_file_encoding(\"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv\")\n",
    "print(f\"ê°ì§€ëœ ì¸ì½”ë”©: {encoding_info}\")\n",
    "\n",
    "# ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸°\n",
    "df = pd.read_csv(\"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv\", encoding=encoding_info['encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7102676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8-SIG ì¸ì½”ë”©ìœ¼ë¡œ ì›ë³¸ íŒŒì¼ ì½ëŠ” ì¤‘...\n",
      "ì›ë³¸ ë°ì´í„°: 5597ê°œ\n",
      "ì»¬ëŸ¼ëª…: ['ë¶„ë¥˜', 'ìì¹˜êµ¬', 'ê³µì—°/í–‰ì‚¬ëª…', 'ë‚ ì§œ/ì‹œê°„', 'ì¥ì†Œ', 'ê¸°ê´€ëª…', 'ì´ìš©ëŒ€ìƒ', 'ì´ìš©ìš”ê¸ˆ', 'ì¶œì—°ìì •ë³´', 'í”„ë¡œê·¸ë¨ì†Œê°œ', 'ê¸°íƒ€ë‚´ìš©', 'í™ˆí˜ì´ì§€?ì£¼ì†Œ', 'ëŒ€í‘œì´ë¯¸ì§€', 'ì‹ ì²­ì¼', 'ì‹œë¯¼/ê¸°ê´€', 'ì‹œì‘ì¼', 'ì¢…ë£Œì¼', 'í…Œë§ˆë¶„ë¥˜', 'ìœ„ë„(Yì¢Œí‘œ)', 'ê²½ë„(Xì¢Œí‘œ)', 'ìœ ë¬´ë£Œ', 'ë¬¸í™”í¬í„¸ìƒì„¸URL']\n",
      "\n",
      "ë¶„ë¥˜ ì»¬ëŸ¼ ìƒ˜í”Œ: ['ì „ì‹œ/ë¯¸ìˆ ', 'í´ë˜ì‹', 'ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ ', 'ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ ', 'ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ ']\n",
      "ìœ ë¬´ë£Œ ì»¬ëŸ¼ ìƒ˜í”Œ: ['ìœ ë£Œ', 'ìœ ë£Œ', 'ë¬´ë£Œ', 'ë¬´ë£Œ', 'ë¬´ë£Œ']\n",
      "ìœ„ë„, ê²½ë„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜/ìœ ë¬´ë£Œ ì •ë³´ ë§¤ì¹­ ì¤‘...\n",
      "\n",
      "ë§¤ì¹­ ì„±ê³µ: 5596ê°œ\n",
      "\n",
      "ë¶„ë¥˜ë³„ í†µê³„:\n",
      "êµìœ¡/ì²´í—˜       2240\n",
      "í´ë˜ì‹         1183\n",
      "ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ      431\n",
      "ì „ì‹œ/ë¯¸ìˆ         411\n",
      "ì½˜ì„œíŠ¸          351\n",
      "ì—°ê·¹           264\n",
      "ì˜í™”           190\n",
      "ê¸°íƒ€           165\n",
      "êµ­ì•…           101\n",
      "ë®¤ì§€ì»¬/ì˜¤í˜ë¼       92\n",
      "Name: ë¶„ë¥˜, dtype: int64\n",
      "\n",
      "ìœ ë¬´ë£Œ í†µê³„:\n",
      "ë¬´ë£Œ    3719\n",
      "ìœ ë£Œ    1877\n",
      "         1\n",
      "Name: ìœ ë¬´ë£Œ, dtype: int64\n",
      "\n",
      "âœ… ìµœì¢… ì™„ì„±ëœ íŒŒì¼ì´ 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìµœì¢… ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n",
      "                   ë‚ ì§œ/ì‹œê°„        ë¶„ë¥˜ ìœ ë¬´ë£Œ         í–‰ì •ë™ì½”ë“œ     ì‹œë„   ì‹œêµ°êµ¬   í–‰ì •ë™\n",
      "0  2025-12-18~2025-12-21     ì „ì‹œ/ë¯¸ìˆ   ìœ ë£Œ  1.168058e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ê°•ë‚¨êµ¬  ì‚¼ì„±1ë™\n",
      "1  2025-12-06~2025-12-06       í´ë˜ì‹  ìœ ë£Œ  1.144060e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ë§ˆí¬êµ¬   ëŒ€í¥ë™\n",
      "2  2025-10-18~2025-10-19  ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ   ë¬´ë£Œ  1.154567e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ê¸ˆì²œêµ¬  ì‹œí¥1ë™\n",
      "3  2025-10-03~2025-10-12  ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ   ë¬´ë£Œ  1.121584e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ê´‘ì§„êµ¬  ìì–‘3ë™\n",
      "4  2025-09-27~2025-09-28  ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ   ë¬´ë£Œ  1.135061e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ë…¸ì›êµ¬  í•˜ê³„1ë™\n",
      "5  2025-09-26~2025-09-26       ì½˜ì„œíŠ¸  ìœ ë£Œ  1.123054e+09  ì„œìš¸íŠ¹ë³„ì‹œ  ë™ëŒ€ë¬¸êµ¬   ì œê¸°ë™\n",
      "6  2025-09-20~2025-09-20  ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ   ë¬´ë£Œ  1.121584e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ê´‘ì§„êµ¬  ìì–‘3ë™\n",
      "7  2025-09-17~2025-09-21     ì „ì‹œ/ë¯¸ìˆ   ë¬´ë£Œ  1.150063e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ê°•ì„œêµ¬  ë°©í™”1ë™\n",
      "8  2025-08-21~2025-08-21       í´ë˜ì‹  ìœ ë£Œ  1.156055e+09  ì„œìš¸íŠ¹ë³„ì‹œ  ì˜ë“±í¬êµ¬  ë‹¹ì‚°1ë™\n",
      "9  2025-08-07~2025-08-10     ì „ì‹œ/ë¯¸ìˆ   ìœ ë£Œ  1.168058e+09  ì„œìš¸íŠ¹ë³„ì‹œ   ê°•ë‚¨êµ¬  ì‚¼ì„±1ë™\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. ì›ë³¸ íŒŒì¼ì„ UTF-8-SIG ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸°\n",
    "print(\"UTF-8-SIG ì¸ì½”ë”©ìœ¼ë¡œ ì›ë³¸ íŒŒì¼ ì½ëŠ” ì¤‘...\")\n",
    "original_df = pd.read_csv(\"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv\", encoding='UTF-8-SIG')\n",
    "\n",
    "print(f\"ì›ë³¸ ë°ì´í„°: {len(original_df)}ê°œ\")\n",
    "print(f\"ì»¬ëŸ¼ëª…: {original_df.columns.tolist()}\")\n",
    "\n",
    "# 2. ë¶„ë¥˜ì™€ ìœ ë¬´ë£Œ ì»¬ëŸ¼ í™•ì¸\n",
    "print(f\"\\në¶„ë¥˜ ì»¬ëŸ¼ ìƒ˜í”Œ: {original_df.iloc[:5, 0].tolist()}\")  # 0ë²ˆì§¸ ì»¬ëŸ¼\n",
    "print(f\"ìœ ë¬´ë£Œ ì»¬ëŸ¼ ìƒ˜í”Œ: {original_df.iloc[:5, 20].tolist()}\")  # 20ë²ˆì§¸ ì»¬ëŸ¼\n",
    "\n",
    "# 3. í˜„ì¬ ë¬¸í™”í–‰ì‚¬ ë°ì´í„° ì½ê¸°\n",
    "culture_df = pd.read_csv(\"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# 4. ì›ë³¸ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ\n",
    "original_info = pd.DataFrame({\n",
    "    'category': original_df.iloc[:, 0],      # ë¶„ë¥˜ (0ë²ˆì§¸ ì»¬ëŸ¼)\n",
    "    'free_paid': original_df.iloc[:, 20],    # ìœ ë¬´ë£Œ (20ë²ˆì§¸ ì»¬ëŸ¼)\n",
    "    'latitude': original_df.iloc[:, 18],     # ìœ„ë„ (18ë²ˆì§¸ ì»¬ëŸ¼)\n",
    "    'longitude': original_df.iloc[:, 19]     # ê²½ë„ (19ë²ˆì§¸ ì»¬ëŸ¼)\n",
    "})\n",
    "\n",
    "# 5. ë§¤ì¹­í•˜ì—¬ ë¶„ë¥˜/ìœ ë¬´ë£Œ ì •ë³´ ì¶”ê°€\n",
    "culture_df['ë¶„ë¥˜'] = ''\n",
    "culture_df['ìœ ë¬´ë£Œ'] = ''\n",
    "\n",
    "match_count = 0\n",
    "\n",
    "print(\"ìœ„ë„, ê²½ë„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜/ìœ ë¬´ë£Œ ì •ë³´ ë§¤ì¹­ ì¤‘...\")\n",
    "\n",
    "for idx, row in culture_df.iterrows():\n",
    "    current_lat = row['ìœ„ë„(Yì¢Œí‘œ)']\n",
    "    current_lng = row['ê²½ë„(Xì¢Œí‘œ)']\n",
    "    \n",
    "    # ë§¤ì¹­ë˜ëŠ” ì›ë³¸ ë°ì´í„° ì°¾ê¸°\n",
    "    matching_rows = original_info[\n",
    "        (abs(original_info['latitude'] - current_lat) < 0.00001) &\n",
    "        (abs(original_info['longitude'] - current_lng) < 0.00001)\n",
    "    ]\n",
    "    \n",
    "    if len(matching_rows) > 0:\n",
    "        match = matching_rows.iloc[0]\n",
    "        culture_df.at[idx, 'ë¶„ë¥˜'] = str(match['category'])\n",
    "        culture_df.at[idx, 'ìœ ë¬´ë£Œ'] = str(match['free_paid'])\n",
    "        match_count += 1\n",
    "\n",
    "print(f\"\\në§¤ì¹­ ì„±ê³µ: {match_count}ê°œ\")\n",
    "\n",
    "# 6. ê²°ê³¼ í™•ì¸\n",
    "print(f\"\\në¶„ë¥˜ë³„ í†µê³„:\")\n",
    "category_counts = culture_df['ë¶„ë¥˜'].value_counts()\n",
    "print(category_counts.head(10))\n",
    "\n",
    "print(f\"\\nìœ ë¬´ë£Œ í†µê³„:\")\n",
    "fee_counts = culture_df['ìœ ë¬´ë£Œ'].value_counts()\n",
    "print(fee_counts)\n",
    "\n",
    "# 7. ìµœì¢… íŒŒì¼ ì €ì¥\n",
    "output_file = \"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±.csv\"\n",
    "culture_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… ìµœì¢… ì™„ì„±ëœ íŒŒì¼ì´ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 8. ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "print(f\"\\nìµœì¢… ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "print(culture_df[['ë‚ ì§œ/ì‹œê°„', 'ë¶„ë¥˜', 'ìœ ë¬´ë£Œ', 'í–‰ì •ë™ì½”ë“œ', 'ì‹œë„', 'ì‹œêµ°êµ¬', 'í–‰ì •ë™']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef16105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼:\n",
      "  - .ipynb_checkpoints\n",
      "  - data merging.ipynb\n",
      "  - ë³‘í•©ëœ_ë°ì´í„°ì…‹_ë¬¸í™”í–‰ì‚¬_ìƒì„¸.csv\n",
      "  - ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv\n",
      "  - ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv\n",
      "  - ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±.csv\n",
      "\n",
      "ë¬¸í™”í–‰ì‚¬ ê´€ë ¨ íŒŒì¼: ['ë³‘í•©ëœ_ë°ì´í„°ì…‹_ë¬¸í™”í–‰ì‚¬_ìƒì„¸.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ëª¨ë“  íŒŒì¼ ëª©ë¡ í™•ì¸\n",
    "all_files = os.listdir('.')\n",
    "print(\"í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼:\")\n",
    "for file in all_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# ë¬¸í™”í–‰ì‚¬ ê´€ë ¨ íŒŒì¼ ì°¾ê¸°\n",
    "culture_files = [f for f in all_files if 'ë¬¸í™”' in f or 'culture' in f.lower()]\n",
    "print(f\"\\në¬¸í™”í–‰ì‚¬ ê´€ë ¨ íŒŒì¼: {culture_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4328170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼:\n",
      "  - .ipynb_checkpoints\n",
      "  - data merging.ipynb\n",
      "  - merged_holidays.csv\n",
      "  - ë³‘í•©ëœ_ë°ì´í„°ì…‹_ë¬¸í™”í–‰ì‚¬_ìƒì„¸.csv\n",
      "  - ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv\n",
      "  - ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv\n",
      "  - ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±_ìˆ˜ì •.csv\n",
      "\n",
      "CSV íŒŒì¼ë“¤: ['merged_holidays.csv', 'ë³‘í•©ëœ_ë°ì´í„°ì…‹_ë¬¸í™”í–‰ì‚¬_ìƒì„¸.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±_ìˆ˜ì •.csv']\n",
      "ë¬¸í™”í–‰ì‚¬ ê´€ë ¨ íŒŒì¼: ['ë³‘í•©ëœ_ë°ì´í„°ì…‹_ë¬¸í™”í–‰ì‚¬_ìƒì„¸.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìœ„ì¹˜ì •ë³´_ì‹œê°„í¬í•¨.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ì •ë³´.csv', 'ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±_ìˆ˜ì •.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ í™•ì¸\n",
    "all_files = os.listdir('.')\n",
    "print(\"í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼:\")\n",
    "for file in all_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# CSV íŒŒì¼ë§Œ í•„í„°ë§\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "print(f\"\\nCSV íŒŒì¼ë“¤: {csv_files}\")\n",
    "\n",
    "# ë¬¸í™”í–‰ì‚¬ ê´€ë ¨ íŒŒì¼ ì°¾ê¸°\n",
    "culture_files = [f for f in all_files if 'ë¬¸í™”' in f or 'culture' in f.lower()]\n",
    "print(f\"ë¬¸í™”í–‰ì‚¬ ê´€ë ¨ íŒŒì¼: {culture_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c539bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "local people dong ë°ì´í„°: 5840ê°œ í–‰\n",
      "ë¬¸í™”í–‰ì‚¬ ë°ì´í„°: 5597ê°œ í–‰ì‚¬\n",
      "local people dongì— ìˆëŠ” ê³ ìœ  í–‰ì •ë™ì½”ë“œ: 16ê°œ\n",
      "í•„í„°ë§ëœ ë¬¸í™”í–‰ì‚¬ ë°ì´í„°: 627ê°œ (ì›ë³¸: 5597ê°œ)\n",
      "ë‚ ì§œ ë²”ìœ„ íŒŒì‹± ë° ì¼ë³„ ë ˆì½”ë“œ ìƒì„± ì¤‘...\n",
      "ìƒì„±ëœ ì¼ë³„ ë¬¸í™”í–‰ì‚¬ ë ˆì½”ë“œ: 11047ê°œ\n",
      "ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ í›„: 7925ê°œ\n",
      "ë°ì´í„° ë³‘í•© ìˆ˜í–‰ ì¤‘...\n",
      "\n",
      "ğŸ“Š ë³‘í•© ê²°ê³¼:\n",
      "ì „ì²´ ë ˆì½”ë“œ: 5,840ê°œ\n",
      "ë¬¸í™”í–‰ì‚¬ê°€ ìˆëŠ” ë ˆì½”ë“œ: 2,319ê°œ (39.71%)\n",
      "ì´ ë¬¸í™”í–‰ì‚¬ ë°œìƒ: 7,925ê±´\n",
      "\n",
      "ğŸ“ˆ ë¬¸í™”í–‰ì‚¬ ë¶„ë¥˜ë³„ ìƒìœ„ 10ê°œ:\n",
      "  ì „ì‹œ/ë¯¸ìˆ : 790ê±´\n",
      "  í´ë˜ì‹: 168ê±´\n",
      "  êµìœ¡/ì²´í—˜: 134ê±´\n",
      "  êµìœ¡/ì²´í—˜|ì „ì‹œ/ë¯¸ìˆ : 79ê±´\n",
      "  êµìœ¡/ì²´í—˜|ì—°ê·¹: 71ê±´\n",
      "  ì—°ê·¹|êµìœ¡/ì²´í—˜: 66ê±´\n",
      "  ì¶•ì œ-ì‹œë¯¼í™”í•©: 66ê±´\n",
      "  ì—°ê·¹: 59ê±´\n",
      "  ì—°ê·¹|ì „ì‹œ/ë¯¸ìˆ : 51ê±´\n",
      "  êµìœ¡/ì²´í—˜|ë®¤ì§€ì»¬/ì˜¤í˜ë¼: 50ê±´\n",
      "\n",
      "ğŸ’° ìœ ë¬´ë£Œ í†µê³„:\n",
      "  ë¬´ë£Œ: 1292ê±´\n",
      "  ìœ ë£Œ: 481ê±´\n",
      "  ë¬´ë£Œ|ìœ ë£Œ: 412ê±´\n",
      "  ìœ ë£Œ|ë¬´ë£Œ: 134ê±´\n",
      "\n",
      "ğŸ’¾ ë³‘í•©ëœ ë°ì´í„°ê°€ 'LOCAL_PEOPLE_DONG_with_culture.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“‹ ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸í™”í–‰ì‚¬ ê´€ë ¨ ì»¬ëŸ¼:\n",
      "  ['has_culture_event', 'culture_event_count', 'ë¶„ë¥˜', 'ìœ ë¬´ë£Œ']\n",
      "\n",
      "ğŸ“‹ ìµœì¢… ë°ì´í„° ì»¬ëŸ¼:\n",
      "['STDR_DE_ID', 'TMZON_PD_SE', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'date', 'culture_event_count', 'ë¶„ë¥˜', 'ìœ ë¬´ë£Œ', 'has_culture_event']\n",
      "\n",
      "ğŸ“‹ ë¬¸í™”í–‰ì‚¬ê°€ ìˆëŠ” ìƒ˜í”Œ ë°ì´í„°:\n",
      "     STDR_DE_ID  ADSTRD_CODE_SE  TOT_LVPOP_CO  culture_event_count  \\\n",
      "480    20240531        11110615   127308.8305                    1   \n",
      "481    20240531        11110650    38217.7573                    1   \n",
      "496    20240601        11110615   107692.2846                    4   \n",
      "497    20240601        11110650    39200.5202                    1   \n",
      "498    20240601        11140550    68181.7737                    1   \n",
      "\n",
      "                 ë¶„ë¥˜    ìœ ë¬´ë£Œ  \n",
      "480             ì½˜ì„œíŠ¸     ë¬´ë£Œ  \n",
      "481              ì—°ê·¹     ë¬´ë£Œ  \n",
      "496  êµìœ¡/ì²´í—˜|ì¶•ì œ-ë¬¸í™”/ì˜ˆìˆ   ìœ ë£Œ|ë¬´ë£Œ  \n",
      "497              ì—°ê·¹     ë¬´ë£Œ  \n",
      "498              ê¸°íƒ€     ë¬´ë£Œ  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_date_range(date_str):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ ë²”ìœ„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ë°˜í™˜\n",
    "    ì˜ˆ: \"2025-06-28~2025-06-28\" -> (datetime(2025,6,28), datetime(2025,6,28))\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        date_str = str(date_str).strip()\n",
    "        if '~' in date_str:\n",
    "            start_str, end_str = date_str.split('~')\n",
    "            start_date = pd.to_datetime(start_str.strip())\n",
    "            end_date = pd.to_datetime(end_str.strip())\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            single_date = pd.to_datetime(date_str)\n",
    "            return single_date, single_date\n",
    "    except Exception as e:\n",
    "        print(f\"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str} - {e}\")\n",
    "        return None, None\n",
    "\n",
    "def merge_culture_with_local_people():\n",
    "    \"\"\"\n",
    "    ë¬¸í™”í–‰ì‚¬ ë°ì´í„°ë¥¼ local people dong ë°ì´í„°ì…‹ê³¼ ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜ìœ¼ë¡œ ë³‘í•©\n",
    "    \"\"\"\n",
    "    print(\"ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    # 1. local people dong ë°ì´í„° ë¡œë“œ\n",
    "    local_people_df = pd.read_csv(\"LOCAL_PEOPLE_DONG_202405_202504_filtered_max.csv\")\n",
    "    print(f\"local people dong ë°ì´í„°: {len(local_people_df)}ê°œ í–‰\")\n",
    "    \n",
    "    # 2. ë¬¸í™”í–‰ì‚¬ ë°ì´í„° ë¡œë“œ\n",
    "    culture_df = pd.read_csv(\"ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±_ìˆ˜ì •.csv\", encoding='utf-8-sig')\n",
    "    print(f\"ë¬¸í™”í–‰ì‚¬ ë°ì´í„°: {len(culture_df)}ê°œ í–‰ì‚¬\")\n",
    "    \n",
    "    # 3. local people dong ë°ì´í„°ì˜ ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "    local_people_df['date'] = pd.to_datetime(local_people_df['STDR_DE_ID'], format='%Y%m%d')\n",
    "    \n",
    "    # 4. í–‰ì •ë™ì½”ë“œ í˜•ì‹ í†µì¼ (8ìë¦¬)\n",
    "    local_people_df['admin_code_8'] = local_people_df['ADSTRD_CODE_SE'].astype(str).str[:8]\n",
    "    culture_df['admin_code_8'] = culture_df['í–‰ì •ë™ì½”ë“œ'].astype(str).str[:8]\n",
    "    \n",
    "    # 5. local people dongì— ìˆëŠ” í–‰ì •ë™ì½”ë“œë§Œ ì¶”ì¶œ (íš¨ìœ¨ì„±ì„ ìœ„í•´)\n",
    "    valid_admin_codes = set(local_people_df['admin_code_8'].unique())\n",
    "    print(f\"local people dongì— ìˆëŠ” ê³ ìœ  í–‰ì •ë™ì½”ë“œ: {len(valid_admin_codes)}ê°œ\")\n",
    "    \n",
    "    # 6. ë¬¸í™”í–‰ì‚¬ ë°ì´í„°ë¥¼ valid_admin_codesë¡œ í•„í„°ë§\n",
    "    culture_filtered = culture_df[culture_df['admin_code_8'].isin(valid_admin_codes)]\n",
    "    print(f\"í•„í„°ë§ëœ ë¬¸í™”í–‰ì‚¬ ë°ì´í„°: {len(culture_filtered)}ê°œ (ì›ë³¸: {len(culture_df)}ê°œ)\")\n",
    "    \n",
    "    if len(culture_filtered) == 0:\n",
    "        print(\"âš ï¸ local people dongê³¼ ë§¤ì¹­ë˜ëŠ” ë¬¸í™”í–‰ì‚¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        # ê¸°ë³¸ ì»¬ëŸ¼ë“¤ì„ ì¶”ê°€\n",
    "        local_people_df['has_culture_event'] = 0\n",
    "        local_people_df['culture_event_count'] = 0\n",
    "        local_people_df['ë¶„ë¥˜'] = ''\n",
    "        local_people_df['ìœ ë¬´ë£Œ'] = ''\n",
    "        return local_people_df\n",
    "    \n",
    "    # 7. ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜ ë§¤ì¹­ì„ ìœ„í•œ ì¼ë³„ ë ˆì½”ë“œ ìƒì„±\n",
    "    print(\"ë‚ ì§œ ë²”ìœ„ íŒŒì‹± ë° ì¼ë³„ ë ˆì½”ë“œ ìƒì„± ì¤‘...\")\n",
    "    daily_culture_records = []\n",
    "    \n",
    "    for idx, culture_row in culture_filtered.iterrows():\n",
    "        start_date, end_date = parse_date_range(culture_row['ë‚ ì§œ/ì‹œê°„'])\n",
    "        \n",
    "        if start_date is None or end_date is None:\n",
    "            continue\n",
    "            \n",
    "        admin_code = culture_row['admin_code_8']\n",
    "        \n",
    "        # í•´ë‹¹ ê¸°ê°„ì˜ ëª¨ë“  ë‚ ì§œì— ëŒ€í•´ ë ˆì½”ë“œ ìƒì„±\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            daily_culture_records.append({\n",
    "                'date': current_date,\n",
    "                'admin_code_8': admin_code,\n",
    "                'ë¶„ë¥˜': culture_row.get('ë¶„ë¥˜', ''),\n",
    "                'ìœ ë¬´ë£Œ': culture_row.get('ìœ ë¬´ë£Œ', ''),\n",
    "                'has_culture_event': 1\n",
    "            })\n",
    "            current_date += timedelta(days=1)\n",
    "    \n",
    "    # 8. ì¼ë³„ ë¬¸í™”í–‰ì‚¬ DataFrame ìƒì„±\n",
    "    daily_culture_df = pd.DataFrame(daily_culture_records)\n",
    "    \n",
    "    if len(daily_culture_df) == 0:\n",
    "        print(\"âš ï¸ ë§¤ì¹­ ê°€ëŠ¥í•œ ë¬¸í™”í–‰ì‚¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        local_people_df['has_culture_event'] = 0\n",
    "        local_people_df['culture_event_count'] = 0\n",
    "        local_people_df['ë¶„ë¥˜'] = ''\n",
    "        local_people_df['ìœ ë¬´ë£Œ'] = ''\n",
    "        return local_people_df\n",
    "    \n",
    "    print(f\"ìƒì„±ëœ ì¼ë³„ ë¬¸í™”í–‰ì‚¬ ë ˆì½”ë“œ: {len(daily_culture_df)}ê°œ\")\n",
    "    \n",
    "    # 9. local people dongì˜ ë‚ ì§œ ë²”ìœ„ ë‚´ì— ìˆëŠ” ë¬¸í™”í–‰ì‚¬ë§Œ í•„í„°ë§\n",
    "    min_date = local_people_df['date'].min()\n",
    "    max_date = local_people_df['date'].max()\n",
    "    daily_culture_df = daily_culture_df[\n",
    "        (daily_culture_df['date'] >= min_date) & \n",
    "        (daily_culture_df['date'] <= max_date)\n",
    "    ]\n",
    "    print(f\"ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ í›„: {len(daily_culture_df)}ê°œ\")\n",
    "    \n",
    "    # 10. ë‚ ì§œì™€ í–‰ì •ë™ì½”ë“œë³„ë¡œ ë¬¸í™”í–‰ì‚¬ ì§‘ê³„\n",
    "    culture_aggregated = daily_culture_df.groupby(['date', 'admin_code_8']).agg({\n",
    "        'has_culture_event': 'sum',  # í•´ë‹¹ ë‚ ì§œ/ì§€ì—­ì˜ ë¬¸í™”í–‰ì‚¬ ê°œìˆ˜\n",
    "        'ë¶„ë¥˜': lambda x: '|'.join(x.unique()),  # ë¬¸í™”í–‰ì‚¬ ë¶„ë¥˜ë“¤ (ì¤‘ë³µ ì œê±°)\n",
    "        'ìœ ë¬´ë£Œ': lambda x: '|'.join(x.unique())  # ìœ ë¬´ë£Œ ì •ë³´ë“¤ (ì¤‘ë³µ ì œê±°)\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 11. ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "    culture_aggregated.rename(columns={\n",
    "        'has_culture_event': 'culture_event_count'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # 12. local people dong ë°ì´í„°ì™€ ë³‘í•©\n",
    "    print(\"ë°ì´í„° ë³‘í•© ìˆ˜í–‰ ì¤‘...\")\n",
    "    merged_df = local_people_df.merge(\n",
    "        culture_aggregated,\n",
    "        left_on=['date', 'admin_code_8'],\n",
    "        right_on=['date', 'admin_code_8'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 13. ë¬¸í™”í–‰ì‚¬ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "    merged_df['culture_event_count'] = merged_df['culture_event_count'].fillna(0).astype(int)\n",
    "    merged_df['has_culture_event'] = (merged_df['culture_event_count'] > 0).astype(int)\n",
    "    merged_df['ë¶„ë¥˜'] = merged_df['ë¶„ë¥˜'].fillna('')  # ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€\n",
    "    merged_df['ìœ ë¬´ë£Œ'] = merged_df['ìœ ë¬´ë£Œ'].fillna('')  # ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€\n",
    "    \n",
    "    # 14. ì„ì‹œ ì»¬ëŸ¼ ì œê±°\n",
    "    merged_df = merged_df.drop('admin_code_8', axis=1)\n",
    "    \n",
    "    # 15. ê²°ê³¼ í†µê³„\n",
    "    total_records = len(merged_df)\n",
    "    records_with_events = len(merged_df[merged_df['has_culture_event'] == 1])\n",
    "    total_events = merged_df['culture_event_count'].sum()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ë³‘í•© ê²°ê³¼:\")\n",
    "    print(f\"ì „ì²´ ë ˆì½”ë“œ: {total_records:,}ê°œ\")\n",
    "    print(f\"ë¬¸í™”í–‰ì‚¬ê°€ ìˆëŠ” ë ˆì½”ë“œ: {records_with_events:,}ê°œ ({records_with_events/total_records*100:.2f}%)\")\n",
    "    print(f\"ì´ ë¬¸í™”í–‰ì‚¬ ë°œìƒ: {total_events:,}ê±´\")\n",
    "    \n",
    "    # 16. ë¶„ë¥˜ë³„ í†µê³„\n",
    "    if records_with_events > 0:\n",
    "        print(f\"\\nğŸ“ˆ ë¬¸í™”í–‰ì‚¬ ë¶„ë¥˜ë³„ ìƒìœ„ 10ê°œ:\")\n",
    "        category_stats = merged_df[merged_df['has_culture_event'] == 1]['ë¶„ë¥˜'].value_counts().head(10)\n",
    "        for category, count in category_stats.items():\n",
    "            print(f\"  {category}: {count}ê±´\")\n",
    "        \n",
    "        print(f\"\\nğŸ’° ìœ ë¬´ë£Œ í†µê³„:\")\n",
    "        fee_stats = merged_df[merged_df['has_culture_event'] == 1]['ìœ ë¬´ë£Œ'].value_counts()\n",
    "        for fee_type, count in fee_stats.items():\n",
    "            print(f\"  {fee_type}: {count}ê±´\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# ì‹¤í–‰\n",
    "try:\n",
    "    final_df = merge_culture_with_local_people()\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    output_file = \"LOCAL_PEOPLE_DONG_with_culture.csv\"\n",
    "    final_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ ë³‘í•©ëœ ë°ì´í„°ê°€ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(f\"\\nğŸ“‹ ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸í™”í–‰ì‚¬ ê´€ë ¨ ì»¬ëŸ¼:\")\n",
    "    culture_columns = ['has_culture_event', 'culture_event_count', 'ë¶„ë¥˜', 'ìœ ë¬´ë£Œ']\n",
    "    print(f\"  {culture_columns}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ìµœì¢… ë°ì´í„° ì»¬ëŸ¼:\")\n",
    "    print(final_df.columns.tolist())\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ë¬¸í™”í–‰ì‚¬ê°€ ìˆëŠ” ìƒ˜í”Œ ë°ì´í„°:\")\n",
    "    sample_with_culture = final_df[final_df['has_culture_event'] == 1].head(5)\n",
    "    display_cols = ['STDR_DE_ID', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'culture_event_count', 'ë¶„ë¥˜', 'ìœ ë¬´ë£Œ']\n",
    "    available_cols = [col for col in display_cols if col in final_df.columns]\n",
    "    if len(sample_with_culture) > 0:\n",
    "        print(sample_with_culture[available_cols])\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n",
    "    print(\"ë‹¤ìŒ íŒŒì¼ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤:\")\n",
    "    print(\"1. LOCAL_PEOPLE_DONG_202405_202504_filtered_max.csv\")\n",
    "    print(\"2. ì„œìš¸ì‹œ_ë¬¸í™”í–‰ì‚¬_ìµœì¢…ì™„ì„±_ìˆ˜ì •.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bd56a",
   "metadata": {},
   "source": [
    "#### ê³µíœ´ì¼ ë°ì´í„° ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a95b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "ìˆ˜ì • ì „ ê³µíœ´ì¼ ë ˆì½”ë“œ: 192ê°œ\n",
      "ìˆ˜ì • í›„ ê³µíœ´ì¼ ë ˆì½”ë“œ: 272ê°œ\n",
      "ì°¨ì´: 80ê°œ\n",
      "2024-12-25 í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ìˆ˜ì • í›„: 1\n",
      "ìˆ˜ì •ëœ ê³µíœ´ì¼ ì •ë³´ ë³‘í•© ì¤‘...\n",
      "\n",
      "ğŸ“Š ìµœì¢… ë³‘í•© ê²°ê³¼:\n",
      "ì „ì²´ ë ˆì½”ë“œ: 5,840ê°œ\n",
      "ê³µíœ´ì¼ ë ˆì½”ë“œ: 272ê°œ (4.66%)\n",
      "\n",
      "ğŸ’¾ ìˆ˜ì •ëœ ê³µíœ´ì¼ ì •ë³´ê°€ í¬í•¨ëœ ìµœì¢… ë°ì´í„°ê°€ 'LOCAL_PEOPLE_DONG_final_corrected.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def merge_holidays_with_corrected_data():\n",
    "    \"\"\"\n",
    "    merged_holidays.csvì˜ is_holidayë¥¼ ìˆ˜ì •í•˜ì—¬ LOCAL_PEOPLE_DONG_with_culture.csvì— ë³‘í•©\n",
    "    \"\"\"\n",
    "    print(\"ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ\n",
    "    holidays_df = pd.read_csv(\"merged_holidays.csv\")\n",
    "    culture_df = pd.read_csv(\"LOCAL_PEOPLE_DONG_with_culture.csv\")\n",
    "    \n",
    "    # 2. 2024-2025ë…„ ë²•ì •ê³µíœ´ì¼ ì •ì˜ (ëˆ„ë½ëœ ê²ƒë“¤ í¬í•¨)\n",
    "    holidays_2024 = [\n",
    "        20240101,  # ì‹ ì •\n",
    "        20240209, 20240210, 20240211, 20240212,  # ì„¤ë‚  ì—°íœ´\n",
    "        20240301,  # ì‚¼ì¼ì ˆ\n",
    "        20240410,  # êµ­íšŒì˜ì›ì„ ê±°ì¼\n",
    "        20240505, 20240506,  # ì–´ë¦°ì´ë‚ \n",
    "        20240515,  # ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚ \n",
    "        20240606,  # í˜„ì¶©ì¼\n",
    "        20240815,  # ê´‘ë³µì ˆ\n",
    "        20240916, 20240917, 20240918,  # ì¶”ì„ ì—°íœ´\n",
    "        20241003,  # ê°œì²œì ˆ\n",
    "        20241009,  # í•œê¸€ë‚ \n",
    "        20241225,  # í¬ë¦¬ìŠ¤ë§ˆìŠ¤ â­ ëˆ„ë½ëœ ê³µíœ´ì¼\n",
    "    ]\n",
    "    \n",
    "    holidays_2025 = [\n",
    "        20250101,  # ì‹ ì •\n",
    "        20250128, 20250129, 20250130,  # ì„¤ë‚  ì—°íœ´\n",
    "        20250301, 20250303,  # ì‚¼ì¼ì ˆ\n",
    "        20250505, 20250506,  # ì–´ë¦°ì´ë‚ , ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚ \n",
    "        20250606,  # í˜„ì¶©ì¼\n",
    "        20250815,  # ê´‘ë³µì ˆ\n",
    "        20251003,  # ê°œì²œì ˆ\n",
    "        20251006, 20251007, 20251008, 20251009,  # ì¶”ì„ ì—°íœ´, í•œê¸€ë‚ \n",
    "        20251225,  # í¬ë¦¬ìŠ¤ë§ˆìŠ¤\n",
    "    ]\n",
    "    \n",
    "    all_holidays = holidays_2024 + holidays_2025\n",
    "    \n",
    "    # 3. ë‚ ì§œ í˜•ì‹ í†µì¼\n",
    "    holidays_df['date_key'] = holidays_df['date'].astype(str)\n",
    "    culture_df['date_key'] = culture_df['STDR_DE_ID'].astype(str)\n",
    "    \n",
    "    # 4. í–‰ì •ë™ì½”ë“œ í˜•ì‹ í†µì¼\n",
    "    holidays_df['admin_key'] = holidays_df['ADSTRD_CODE_SE'].astype(str)\n",
    "    culture_df['admin_key'] = culture_df['ADSTRD_CODE_SE'].astype(str)\n",
    "    \n",
    "    # 5. â­ ê³µíœ´ì¼ ì •ë³´ ìˆ˜ì • (ê¸°ì¡´ is_holiday ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ê³„ì‚°)\n",
    "    holidays_df['is_holiday_corrected'] = holidays_df['date'].isin(all_holidays).astype(int)\n",
    "    \n",
    "    # 6. ìˆ˜ì • ì „í›„ ë¹„êµ\n",
    "    original_holidays = holidays_df['is_holiday'].sum()\n",
    "    corrected_holidays = holidays_df['is_holiday_corrected'].sum()\n",
    "    print(f\"ìˆ˜ì • ì „ ê³µíœ´ì¼ ë ˆì½”ë“œ: {original_holidays:,}ê°œ\")\n",
    "    print(f\"ìˆ˜ì • í›„ ê³µíœ´ì¼ ë ˆì½”ë“œ: {corrected_holidays:,}ê°œ\")\n",
    "    print(f\"ì°¨ì´: {corrected_holidays - original_holidays:,}ê°œ\")\n",
    "    \n",
    "    # 7. í¬ë¦¬ìŠ¤ë§ˆìŠ¤ í™•ì¸\n",
    "    christmas_2024 = holidays_df[holidays_df['date'] == 20241225]['is_holiday_corrected'].iloc[0] if len(holidays_df[holidays_df['date'] == 20241225]) > 0 else \"ë°ì´í„° ì—†ìŒ\"\n",
    "    print(f\"2024-12-25 í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ìˆ˜ì • í›„: {christmas_2024}\")\n",
    "    \n",
    "    # 8. ìˆ˜ì •ëœ ê³µíœ´ì¼ ì •ë³´ë¡œ ë³‘í•©\n",
    "    holiday_info = holidays_df[['date_key', 'admin_key', 'is_holiday_corrected']].drop_duplicates()\n",
    "    holiday_info.rename(columns={'is_holiday_corrected': 'is_holiday'}, inplace=True)\n",
    "    \n",
    "    # 9. LOCAL_PEOPLE_DONG_with_cultureì™€ ë³‘í•©\n",
    "    print(\"ìˆ˜ì •ëœ ê³µíœ´ì¼ ì •ë³´ ë³‘í•© ì¤‘...\")\n",
    "    merged_df = culture_df.merge(\n",
    "        holiday_info,\n",
    "        left_on=['date_key', 'admin_key'],\n",
    "        right_on=['date_key', 'admin_key'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 10. ë³‘í•©ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •\n",
    "    merged_df['is_holiday'] = merged_df['is_holiday'].fillna(0).astype(int)\n",
    "    \n",
    "    # 11. ì„ì‹œ ì»¬ëŸ¼ ì œê±°\n",
    "    merged_df = merged_df.drop(['date_key', 'admin_key'], axis=1)\n",
    "    \n",
    "    # 12. ê²°ê³¼ í†µê³„\n",
    "    total_records = len(merged_df)\n",
    "    holiday_records = len(merged_df[merged_df['is_holiday'] == 1])\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ìµœì¢… ë³‘í•© ê²°ê³¼:\")\n",
    "    print(f\"ì „ì²´ ë ˆì½”ë“œ: {total_records:,}ê°œ\")\n",
    "    print(f\"ê³µíœ´ì¼ ë ˆì½”ë“œ: {holiday_records:,}ê°œ ({holiday_records/total_records*100:.2f}%)\")\n",
    "    \n",
    "    # 13. ê²°ê³¼ ì €ì¥\n",
    "    output_file = \"LOCAL_PEOPLE_DONG_final_corrected.csv\"\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ ìˆ˜ì •ëœ ê³µíœ´ì¼ ì •ë³´ê°€ í¬í•¨ëœ ìµœì¢… ë°ì´í„°ê°€ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# ì‹¤í–‰\n",
    "final_df = merge_holidays_with_corrected_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fadb81",
   "metadata": {},
   "source": [
    "#### ë‚ ì”¨ ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c890dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: 'merged_people_weather_utf8sig.csv' (í•œê¸€ ìœ ì§€ + ê¹¨ì§ ì—†ìŒ)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ìœ ë™ì¸êµ¬ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "people_df = pd.read_csv(\"LOCAL_PEOPLE_DONG_final_corrected.csv\")\n",
    "people_df['date'] = pd.to_datetime(people_df['STDR_DE_ID'].astype(str), format=\"%Y%m%d\")\n",
    "\n",
    "# ë‚ ì”¨ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "weather_df = pd.read_csv(\"OBS_ASOS_DD_20250531150815.csv\", encoding=\"cp949\")\n",
    "weather_df['date'] = pd.to_datetime(weather_df['ì¼ì‹œ'], format=\"%Y-%m-%d\")\n",
    "\n",
    "# ë‚ ì”¨ feature ì¶”ì¶œ ë° ì»¬ëŸ¼ëª… ì˜ì–´ë¡œ\n",
    "weather_selected = weather_df[[\n",
    "    'date', 'í‰ê· ê¸°ì˜¨(Â°C)', 'ìµœê³ ê¸°ì˜¨(Â°C)', 'ìµœì €ê¸°ì˜¨(Â°C)',\n",
    "    'ì¼ê°•ìˆ˜ëŸ‰(mm)', 'í‰ê·  í’ì†(m/s)', 'í•©ê³„ ì¼ì¡°ì‹œê°„(hr)',\n",
    "    'í‰ê·  ìƒëŒ€ìŠµë„(%)', 'í‰ê·  ì§€ë©´ì˜¨ë„(Â°C)'\n",
    "]].copy()\n",
    "\n",
    "weather_selected.columns = [\n",
    "    'date', 'avg_temp', 'max_temp', 'min_temp',\n",
    "    'precipitation', 'wind_speed', 'sunshine_hours',\n",
    "    'humidity', 'ground_temp'\n",
    "]\n",
    "\n",
    "# ë³‘í•©\n",
    "merged_df = pd.merge(people_df, weather_selected, on='date', how='left')\n",
    "\n",
    "# âœ… í•œê¸€ ì»¬ëŸ¼/ê°’ ë³´ì¡´í•˜ë©´ì„œ ì €ì¥ (Excel ë“±ì—ì„œ ê¹¨ì§ ë°©ì§€)\n",
    "merged_df.to_csv(\"merged_people_weather_utf8sig.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… ì €ì¥ ì™„ë£Œ: 'merged_people_weather_utf8sig.csv' (í•œê¸€ ìœ ì§€ + ê¹¨ì§ ì—†ìŒ)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92064a40",
   "metadata": {},
   "source": [
    "### ì§€í•˜ì²  ìŠ¹í•˜ì°¨ êµí†µì •ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "256bd55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ì§€í•˜ì²  ë°ì´í„° êµ¬ì¡° ìˆ˜ì •\n",
      "==================================================\n",
      "ì›ë³¸ ë°ì´í„°: (19110, 6)\n",
      "\n",
      "ğŸ“‹ ìˆ˜ì •ëœ ë°ì´í„° ìƒ˜í”Œ:\n",
      "              ì‚¬ìš©ì¼ì  ë…¸ì„ ëª…        ì—­ëª…  ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜  í•˜ì°¨ì´ìŠ¹ê°ìˆ˜      ë“±ë¡ì¼ì\n",
      "20240501  20240501  ì•ˆì‚°ì„         ì•ˆì‚°   10467    9980  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„         ì´ˆì§€    4052    4244  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„         ê³ ì”    6923    6696  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„         ì¤‘ì•™   17412   17710  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„        í•œëŒ€ì•    9509    9460  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„        ìƒë¡ìˆ˜   12528   12649  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„         ë°˜ì›”    3871    3879  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„        ëŒ€ì•¼ë¯¸    4648    4154  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„         ì‚°ë³¸   14497   15009  20240504\n",
      "20240501  20240501  ì‹ ë¦¼ì„   ê´€ì•…ì‚°(ì„œìš¸ëŒ€)    4358    4908  20240504\n",
      "\n",
      "ğŸ“‹ ë‚ ì§œ ìˆ˜ì • í›„:\n",
      "              ì‚¬ìš©ì¼ì  ë…¸ì„ ëª…   ì—­ëª…  ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜  í•˜ì°¨ì´ìŠ¹ê°ìˆ˜      ë“±ë¡ì¼ì\n",
      "20240501  20240501  ì•ˆì‚°ì„    ì•ˆì‚°   10467    9980  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„    ì´ˆì§€    4052    4244  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„    ê³ ì”    6923    6696  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„    ì¤‘ì•™   17412   17710  20240504\n",
      "20240501  20240501  ì•ˆì‚°ì„   í•œëŒ€ì•    9509    9460  20240504\n",
      "\n",
      "ğŸ“‹ ì‹¤ì œ ì—­ëª… ìƒ˜í”Œ (ì²˜ìŒ 20ê°œ):\n",
      "   1. ì•ˆì‚°\n",
      "   2. ì´ˆì§€\n",
      "   3. ê³ ì”\n",
      "   4. ì¤‘ì•™\n",
      "   5. í•œëŒ€ì•\n",
      "   6. ìƒë¡ìˆ˜\n",
      "   7. ë°˜ì›”\n",
      "   8. ëŒ€ì•¼ë¯¸\n",
      "   9. ì‚°ë³¸\n",
      "  10. ê´€ì•…ì‚°(ì„œìš¸ëŒ€)\n",
      "  11. ì„œìš¸ëŒ€ë²¤ì²˜íƒ€ìš´\n",
      "  12. ì„œì›\n",
      "  13. ì‹ ë¦¼\n",
      "  14. ë‹¹ê³¡\n",
      "  15. ë³´ë¼ë§¤ë³‘ì›\n",
      "  16. ë³´ë¼ë§¤ê³µì›\n",
      "  17. ë³´ë¼ë§¤\n",
      "  18. ì„œìš¸ì§€ë°©ë³‘ë¬´ì²­\n",
      "  19. ëŒ€ë°©\n",
      "  20. ìƒ›ê°•\n"
     ]
    }
   ],
   "source": [
    "def fix_subway_data_structure():\n",
    "    \"\"\"\n",
    "    ë’¤ì„ì¸ ì§€í•˜ì²  ë°ì´í„° êµ¬ì¡°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¬êµ¬ì„±\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ ì§€í•˜ì²  ë°ì´í„° êµ¬ì¡° ìˆ˜ì •\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "    df = pd.read_csv(\"subway/CARD_SUBWAY_MONTH_202405.csv\", \n",
    "                    encoding='utf-8-sig',\n",
    "                    error_bad_lines=False)\n",
    "    \n",
    "    print(f\"ì›ë³¸ ë°ì´í„°: {df.shape}\")\n",
    "    \n",
    "    # 2. ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ ë§¤í•‘ìœ¼ë¡œ ì¬êµ¬ì„±\n",
    "    df_fixed = pd.DataFrame({\n",
    "        'ì‚¬ìš©ì¼ì': df.index,  # ì¸ë±ìŠ¤ê°€ ì‹¤ì œ ë‚ ì§œì¸ ê²ƒ ê°™ìŒ\n",
    "        'ë…¸ì„ ëª…': df['ì‚¬ìš©ì¼ì'],  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹¤ì œ ë…¸ì„ ëª…\n",
    "        'ì—­ëª…': df['ë…¸ì„ ëª…'],     # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹¤ì œ ì—­ëª…\n",
    "        'ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜': df['ì—­ëª…'],    # ì„¸ ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹¤ì œ ìŠ¹ì°¨ìŠ¹ê°ìˆ˜\n",
    "        'í•˜ì°¨ì´ìŠ¹ê°ìˆ˜': df['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'], # ë„¤ ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹¤ì œ í•˜ì°¨ìŠ¹ê°ìˆ˜\n",
    "        'ë“±ë¡ì¼ì': df['í•˜ì°¨ì´ìŠ¹ê°ìˆ˜']   # ë‹¤ì„¯ ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹¤ì œ ë“±ë¡ì¼ì\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ìˆ˜ì •ëœ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "    print(df_fixed.head(10))\n",
    "    \n",
    "    # 3. ì‹¤ì œ ë‚ ì§œ ì¶”ì¶œ (ì¸ë±ìŠ¤ì—ì„œ)\n",
    "    # ì²« ë²ˆì§¸ í–‰ì˜ ì¸ë±ìŠ¤ê°€ 20240501ì´ë¯€ë¡œ ì´ë¥¼ í™œìš©\n",
    "    first_date = 20240501\n",
    "    df_fixed['ì‚¬ìš©ì¼ì'] = first_date  # ëª¨ë“  í–‰ì´ ê°™ì€ ë‚ ì§œ\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ë‚ ì§œ ìˆ˜ì • í›„:\")\n",
    "    print(df_fixed.head())\n",
    "    \n",
    "    # 4. ì—­ëª… í™•ì¸\n",
    "    print(f\"\\nğŸ“‹ ì‹¤ì œ ì—­ëª… ìƒ˜í”Œ (ì²˜ìŒ 20ê°œ):\")\n",
    "    unique_stations = df_fixed['ì—­ëª…'].unique()\n",
    "    for i, station in enumerate(unique_stations[:20]):\n",
    "        print(f\"  {i+1:2d}. {station}\")\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# ì‹¤í–‰\n",
    "fixed_subway_data = fix_subway_data_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7328763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ 2025ë…„ 2ì›” íŒŒì¼ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°\n",
      "==================================================\n",
      "íŒŒì¼ ì‹œì‘ ë°”ì´íŠ¸: b'\\xbb\\xe7\\xbf\\xeb\\xc0\\xcf\\xc0\\xda,\\xb3'\n",
      "â“ BOM ì—†ìŒ, ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„\n",
      "ì‹œë„ ì¤‘: utf-8-sig\n",
      "âŒ utf-8-sig: ìœ ë‹ˆì½”ë“œ ì˜¤ë¥˜ - 'utf-8' codec can't decode byte 0xbb in position 0...\n",
      "ì‹œë„ ì¤‘: utf-8\n",
      "âŒ utf-8: ìœ ë‹ˆì½”ë“œ ì˜¤ë¥˜ - 'utf-8' codec can't decode byte 0xbb in position 0...\n",
      "ì‹œë„ ì¤‘: euc-kr\n",
      "âœ… euc-kr ì¸ì½”ë”©ìœ¼ë¡œ ì„±ê³µ!\n",
      "ë°ì´í„° í¬ê¸°: (17277, 6)\n",
      "ì»¬ëŸ¼: ['ì‚¬ìš©ì¼ì', 'ë…¸ì„ ëª…', 'ì—­ëª…', 'ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜', 'í•˜ì°¨ì´ìŠ¹ê°ìˆ˜', 'ë“±ë¡ì¼ì']\n",
      "\n",
      "ğŸ“‹ 2ì›” ë°ì´í„° ìƒ˜í”Œ:\n",
      "       ì‚¬ìš©ì¼ì  ë…¸ì„ ëª…    ì—­ëª…  ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜  í•˜ì°¨ì´ìŠ¹ê°ìˆ˜      ë“±ë¡ì¼ì\n",
      "0  20250201  1í˜¸ì„    ì„œìš¸ì—­   56802   45819  20250204\n",
      "1  20250201  1í˜¸ì„     ì‹œì²­   31681   29724  20250204\n",
      "2  20250201  1í˜¸ì„     ì¢…ê°   27189   25388  20250204\n",
      "3  20250201  1í˜¸ì„   ì¢…ë¡œ3ê°€   25776   23449  20250204\n",
      "4  20250201  1í˜¸ì„   ì¢…ë¡œ5ê°€   20211   20055  20250204\n"
     ]
    }
   ],
   "source": [
    "def fix_202502_encoding_issue():\n",
    "    \"\"\"\n",
    "    2025ë…„ 2ì›” íŒŒì¼ ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ 2025ë…„ 2ì›” íŒŒì¼ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    file_path = \"subway/CARD_SUBWAY_MONTH_202502.csv\"\n",
    "    \n",
    "    # 1. ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ íŒŒì¼ ì½ì–´ì„œ BOM í™•ì¸\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_bytes = f.read(10)\n",
    "            print(f\"íŒŒì¼ ì‹œì‘ ë°”ì´íŠ¸: {raw_bytes}\")\n",
    "            \n",
    "            # BOM íŒ¨í„´ í™•ì¸\n",
    "            if raw_bytes.startswith(b'\\xef\\xbb\\xbf'):\n",
    "                print(\"âœ… UTF-8 BOM ê°ì§€\")\n",
    "                encoding = 'utf-8-sig'\n",
    "            elif raw_bytes.startswith(b'\\xff\\xfe'):\n",
    "                print(\"âœ… UTF-16 LE BOM ê°ì§€\")\n",
    "                encoding = 'utf-16'\n",
    "            elif raw_bytes.startswith(b'\\xfe\\xff'):\n",
    "                print(\"âœ… UTF-16 BE BOM ê°ì§€\")\n",
    "                encoding = 'utf-16'\n",
    "            else:\n",
    "                print(\"â“ BOM ì—†ìŒ, ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„\")\n",
    "                encoding = None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„\n",
    "    encodings_to_try = [\n",
    "        'utf-8-sig', 'utf-8', 'euc-kr', 'cp949', 'latin1', \n",
    "        'iso-8859-1', 'utf-16', 'utf-16le', 'utf-16be'\n",
    "    ]\n",
    "    \n",
    "    if encoding:\n",
    "        encodings_to_try.insert(0, encoding)\n",
    "    \n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            print(f\"ì‹œë„ ì¤‘: {enc}\")\n",
    "            \n",
    "            # ê¸°ë³¸ ë¡œë“œ ì‹œë„\n",
    "            df = pd.read_csv(file_path, encoding=enc)\n",
    "            print(f\"âœ… {enc} ì¸ì½”ë”©ìœ¼ë¡œ ì„±ê³µ!\")\n",
    "            print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "            print(f\"ì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "            return df\n",
    "            \n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"âŒ {enc}: ìœ ë‹ˆì½”ë“œ ì˜¤ë¥˜ - {str(e)[:50]}...\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {enc}: ê¸°íƒ€ ì˜¤ë¥˜ - {str(e)[:50]}...\")\n",
    "            continue\n",
    "    \n",
    "    # 3. ë§ˆì§€ë§‰ ì‹œë„: ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ë¡œë“œ\n",
    "    try:\n",
    "        print(\"\\nğŸ”„ ì˜¤ë¥˜ ë¬´ì‹œ ëª¨ë“œë¡œ ì‹œë„...\")\n",
    "        df = pd.read_csv(file_path, \n",
    "                        encoding='utf-8', \n",
    "                        errors='ignore',\n",
    "                        on_bad_lines='skip')\n",
    "        print(f\"âœ… ì˜¤ë¥˜ ë¬´ì‹œ ëª¨ë“œ ì„±ê³µ!\")\n",
    "        print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë¬´ì‹œ ëª¨ë“œë„ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # 4. ìµœí›„ì˜ ìˆ˜ë‹¨: í…ìŠ¤íŠ¸ ì—ë””í„°ë¡œ í™•ì¸ í•„ìš”\n",
    "    print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "    print(\"1. íŒŒì¼ì„ ë©”ëª¨ì¥ì´ë‚˜ í…ìŠ¤íŠ¸ ì—ë””í„°ë¡œ ì—´ì–´ì„œ ì¸ì½”ë”© í™•ì¸\")\n",
    "    print(\"2. ë‹¤ë¥¸ ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥ í›„ ì¬ì‹œë„\")\n",
    "    print(\"3. ë˜ëŠ” í•´ë‹¹ ì›” ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "feb_data = fix_202502_encoding_issue()\n",
    "\n",
    "if feb_data is not None:\n",
    "    print(f\"\\nğŸ“‹ 2ì›” ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "    print(feb_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49bc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš‡ ì™„ì „í•œ 12ê°œì›” ì§€í•˜ì²  ë°ì´í„° ì²˜ë¦¬\n",
      "==================================================\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202405.csv: 1519ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202406.csv: 1470ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202407.csv: 1519ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202408.csv: 1519ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202409.csv: 1470ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202410.csv: 1519ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202411.csv: 1470ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202412.csv: 1519ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202501.csv: 1519ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202502.csv: 1372ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202503.csv: 1519ê°œ í–‰ ì¶”ì¶œ\n",
      "âœ… subway/CARD_SUBWAY_MONTH_202504.csv: 1422ê°œ í–‰ ì¶”ì¶œ\n",
      "\n",
      "ğŸ“Š ì™„ì „í•œ 12ê°œì›” ì¶”ì¶œ ê²°ê³¼:\n",
      "ì´ ë°ì´í„°: 17,837ê°œ í–‰\n",
      "ê³ ìœ  ì—­: 31ê°œ\n",
      "ê³ ìœ  í–‰ì •ë™: 16ê°œ\n",
      "ë‚ ì§œ ë²”ìœ„: 20240501 ~ 20250430\n",
      "\n",
      "ğŸ’¾ ì™„ì „í•œ 12ê°œì›” ë°ì´í„°ê°€ 'complete_12months_subway_data.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "def process_complete_12_months_subway_data():\n",
    "    \"\"\"\n",
    "    2ì›” í¬í•¨ ì™„ì „í•œ 12ê°œì›” ì§€í•˜ì²  ë°ì´í„° ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "    print(\"ğŸš‡ ì™„ì „í•œ 12ê°œì›” ì§€í•˜ì²  ë°ì´í„° ì²˜ë¦¬\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ì¡°ì‚¬ ëŒ€ìƒ í–‰ì •ë™ ì§€í•˜ì² ì—­ ë§¤í•‘\n",
    "    station_mapping = {\n",
    "        'ì¢…ê°': '11110615', 'ì¢…ë¡œ3ê°€': '11110615', 'ê´‘í™”ë¬¸': '11110615',\n",
    "        'ì„ì§€ë¡œ3ê°€': '11110615', 'ì¢…ë¡œ5ê°€': '11110615', 'ë™ë¬˜ì•': '11110650',\n",
    "        'ëª…ë™': '11140550', 'ì„ì§€ë¡œì…êµ¬': '11140550', 'ì„ì§€ë¡œ4ê°€': '11140550',\n",
    "        'ë™ëŒ€ë¬¸': '11140605', 'ë™ëŒ€ë¬¸ì—­ì‚¬ë¬¸í™”ê³µì›': '11140605',\n",
    "        'ìš©ì‚°': '11170520', 'í•œê°•ì§„': '11170520', 'ì‚¼ê°ì§€': '11170520',\n",
    "        'ì´íƒœì›': '11170650', 'ëšì„¬': '11200660', 'ê±´ëŒ€ì…êµ¬': '11200660',\n",
    "        'ì„±ìˆ˜': '11200690', 'í•©ì •': '11440660', 'ìƒìˆ˜': '11440690',\n",
    "        'í™ëŒ€ì…êµ¬': '11440710', 'ë§ì›': '11440710', 'ì‹ ì´Œ': '11440710',\n",
    "        'ì—¬ì˜ë„': '11560605', 'ë‹¹ì‚°': '11560605', 'ì˜ë“±í¬êµ¬ì²­': '11560605',\n",
    "        'ì‹ ë¦¼': '11620745', 'ë´‰ì²œ': '11620745', 'ê°•ë‚¨': '11680510',\n",
    "        'ì—­ì‚¼': '11680640', 'ì„ ë¦‰': '11680640', 'ì••êµ¬ì •': '11680640',\n",
    "        'ì ì‹¤': '11710580', 'ì„ì´Œ': '11710580'\n",
    "    }\n",
    "    \n",
    "    # ëª¨ë“  ì›”ë³„ íŒŒì¼ ì²˜ë¦¬ (2ì›” í¬í•¨)\n",
    "    subway_files = [\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202405.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202406.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202407.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202408.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202409.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202410.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202411.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202412.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202501.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202502.csv\", 'euc-kr'),  # 2ì›”ì€ EUC-KR\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202503.csv\", 'utf-8-sig'),\n",
    "        (\"subway/CARD_SUBWAY_MONTH_202504.csv\", 'utf-8-sig')\n",
    "    ]\n",
    "    \n",
    "    all_subway_data = []\n",
    "    \n",
    "    for file_path, encoding in subway_files:\n",
    "        try:\n",
    "            # íŒŒì¼ë³„ ì ì ˆí•œ ì¸ì½”ë”©ìœ¼ë¡œ ë¡œë“œ\n",
    "            df = pd.read_csv(file_path, encoding=encoding, error_bad_lines=False)\n",
    "            \n",
    "            # 2ì›” ë°ì´í„°ëŠ” ì´ë¯¸ ì˜¬ë°”ë¥¸ êµ¬ì¡°\n",
    "            if '202502' in file_path:\n",
    "                df_processed = df.copy()\n",
    "                # ë‚ ì§œ ì»¬ëŸ¼ì´ ì´ë¯¸ ì˜¬ë°”ë¦„\n",
    "            else:\n",
    "                # ë‹¤ë¥¸ ì›”ë“¤ì€ êµ¬ì¡° ì¬ì •ë ¬ í•„ìš”\n",
    "                df_processed = pd.DataFrame({\n",
    "                    'ì‚¬ìš©ì¼ì': df.index.map(lambda x: int(str(x)[:8]) if len(str(x)) >= 8 else 20240501),\n",
    "                    'ë…¸ì„ ëª…': df['ì‚¬ìš©ì¼ì'],\n",
    "                    'ì—­ëª…': df['ë…¸ì„ ëª…'],\n",
    "                    'ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜': df['ì—­ëª…'],\n",
    "                    'í•˜ì°¨ì´ìŠ¹ê°ìˆ˜': df['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'],\n",
    "                    'ë“±ë¡ì¼ì': df['í•˜ì°¨ì´ìŠ¹ê°ìˆ˜']\n",
    "                })\n",
    "            \n",
    "            # ì¡°ì‚¬ ëŒ€ìƒ ì—­ë§Œ í•„í„°ë§\n",
    "            target_stations = list(station_mapping.keys())\n",
    "            df_target = df_processed[df_processed['ì—­ëª…'].isin(target_stations)].copy()\n",
    "            \n",
    "            # í–‰ì •ë™ì½”ë“œ ì¶”ê°€\n",
    "            df_target['í–‰ì •ë™ì½”ë“œ'] = df_target['ì—­ëª…'].map(station_mapping)\n",
    "            \n",
    "            if len(df_target) > 0:\n",
    "                all_subway_data.append(df_target)\n",
    "                print(f\"âœ… {file_path}: {len(df_target)}ê°œ í–‰ ì¶”ì¶œ\")\n",
    "            else:\n",
    "                print(f\"âŒ {file_path}: ì¡°ì‚¬ ëŒ€ìƒ ì—­ ì—†ìŒ\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {file_path} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # ëª¨ë“  ë°ì´í„° í†µí•©\n",
    "    if all_subway_data:\n",
    "        final_subway_data = pd.concat(all_subway_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì™„ì „í•œ 12ê°œì›” ì¶”ì¶œ ê²°ê³¼:\")\n",
    "        print(f\"ì´ ë°ì´í„°: {len(final_subway_data):,}ê°œ í–‰\")\n",
    "        print(f\"ê³ ìœ  ì—­: {final_subway_data['ì—­ëª…'].nunique()}ê°œ\")\n",
    "        print(f\"ê³ ìœ  í–‰ì •ë™: {final_subway_data['í–‰ì •ë™ì½”ë“œ'].nunique()}ê°œ\")\n",
    "        print(f\"ë‚ ì§œ ë²”ìœ„: {final_subway_data['ì‚¬ìš©ì¼ì'].min()} ~ {final_subway_data['ì‚¬ìš©ì¼ì'].max()}\")\n",
    "        \n",
    "        # ë°ì´í„° ì €ì¥\n",
    "        output_file = \"complete_12months_subway_data.csv\"\n",
    "        final_subway_data.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nğŸ’¾ ì™„ì „í•œ 12ê°œì›” ë°ì´í„°ê°€ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return final_subway_data\n",
    "    else:\n",
    "        print(\"âŒ ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "complete_subway_data = process_complete_12_months_subway_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b305542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì§€í•˜ì²  ë°ì´í„° ë§¤í•‘ í’ˆì§ˆ ì²´í¬\n",
      "==================================================\n",
      "ğŸ“Š ì „ì²´ ë°ì´í„° í˜„í™©:\n",
      "ì´ ë°ì´í„°: 17,837ê°œ í–‰\n",
      "ê³ ìœ  ì—­: 31ê°œ\n",
      "ê³ ìœ  í–‰ì •ë™: 16ê°œ\n",
      "ë‚ ì§œ ë²”ìœ„: 20240501 ~ 20250430\n",
      "\n",
      "ğŸ“‹ í–‰ì •ë™ë³„ ë§¤í•‘ í˜„í™©:\n",
      "  11110615: 4ê°œ ì—­, 365ì¼\n",
      "  11110650: 1ê°œ ì—­, 365ì¼\n",
      "  11140550: 3ê°œ ì—­, 365ì¼\n",
      "  11140605: 1ê°œ ì—­, 365ì¼\n",
      "  11170520: 3ê°œ ì—­, 365ì¼\n",
      "  11170650: 1ê°œ ì—­, 365ì¼\n",
      "  11200660: 2ê°œ ì—­, 365ì¼\n",
      "  11200690: 1ê°œ ì—­, 365ì¼\n",
      "  11440660: 1ê°œ ì—­, 365ì¼\n",
      "  11440690: 1ê°œ ì—­, 365ì¼\n",
      "  11440710: 3ê°œ ì—­, 365ì¼\n",
      "  11560605: 3ê°œ ì—­, 365ì¼\n",
      "  11620745: 2ê°œ ì—­, 365ì¼\n",
      "  11680510: 1ê°œ ì—­, 365ì¼\n",
      "  11680640: 3ê°œ ì—­, 365ì¼\n",
      "  11710580: 1ê°œ ì—­, 365ì¼\n",
      "\n",
      "ğŸ“‹ ì—­ë³„ ë°ì´í„° ë¶„í¬:\n",
      "  í™ëŒ€ì…êµ¬ (11440710): 1095ê°œ í–‰\n",
      "  ì¢…ë¡œ3ê°€ (11110615): 1095ê°œ í–‰\n",
      "  ë™ë¬˜ì• (11110650): 730ê°œ í–‰\n",
      "  ì—¬ì˜ë„ (11560605): 730ê°œ í–‰\n",
      "  ë‹¹ì‚° (11560605): 730ê°œ í–‰\n",
      "  ì„ì§€ë¡œ3ê°€ (11110615): 730ê°œ í–‰\n",
      "  ì„ì§€ë¡œ4ê°€ (11140550): 730ê°œ í–‰\n",
      "  ì‹ ì´Œ (11440710): 730ê°œ í–‰\n",
      "  ë™ëŒ€ë¬¸ (11140605): 730ê°œ í–‰\n",
      "  ê±´ëŒ€ì…êµ¬ (11200660): 730ê°œ í–‰\n",
      "  ì„ ë¦‰ (11680640): 730ê°œ í–‰\n",
      "  í•©ì • (11440660): 730ê°œ í–‰\n",
      "  ì˜ë“±í¬êµ¬ì²­ (11560605): 730ê°œ í–‰\n",
      "  ì„ì´Œ (11710580): 730ê°œ í–‰\n",
      "  ì‹ ë¦¼ (11620745): 730ê°œ í–‰\n",
      "  ì‚¼ê°ì§€ (11170520): 682ê°œ í–‰\n",
      "  ì´íƒœì› (11170650): 365ê°œ í–‰\n",
      "  ê°•ë‚¨ (11680510): 365ê°œ í–‰\n",
      "  ìš©ì‚° (11170520): 365ê°œ í–‰\n",
      "  í•œê°•ì§„ (11170520): 365ê°œ í–‰\n",
      "  ì„ì§€ë¡œì…êµ¬ (11140550): 365ê°œ í–‰\n",
      "  ìƒìˆ˜ (11440690): 365ê°œ í–‰\n",
      "  ì¢…ê° (11110615): 365ê°œ í–‰\n",
      "  ë§ì› (11440710): 365ê°œ í–‰\n",
      "  ì—­ì‚¼ (11680640): 365ê°œ í–‰\n",
      "  ëšì„¬ (11200660): 365ê°œ í–‰\n",
      "  ì••êµ¬ì • (11680640): 365ê°œ í–‰\n",
      "  ì„±ìˆ˜ (11200690): 365ê°œ í–‰\n",
      "  ëª…ë™ (11140550): 365ê°œ í–‰\n",
      "  ì¢…ë¡œ5ê°€ (11110615): 365ê°œ í–‰\n",
      "  ë´‰ì²œ (11620745): 365ê°œ í–‰\n",
      "\n",
      "ğŸ“‹ ì›”ë³„ ë°ì´í„° ë¶„í¬:\n",
      "  01ì›”: 1519ê°œ í–‰\n",
      "  02ì›”: 1372ê°œ í–‰\n",
      "  03ì›”: 1519ê°œ í–‰\n",
      "  04ì›”: 1422ê°œ í–‰\n",
      "  05ì›”: 1519ê°œ í–‰\n",
      "  06ì›”: 1470ê°œ í–‰\n",
      "  07ì›”: 1519ê°œ í–‰\n",
      "  08ì›”: 1519ê°œ í–‰\n",
      "  09ì›”: 1470ê°œ í–‰\n",
      "  10ì›”: 1519ê°œ í–‰\n",
      "  11ì›”: 1470ê°œ í–‰\n",
      "  12ì›”: 1519ê°œ í–‰\n",
      "\n",
      "ğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\n",
      "ê²°ì¸¡ì¹˜:\n",
      "  ì—­ëª…: 0ê°œ\n",
      "  í–‰ì •ë™ì½”ë“œ: 0ê°œ\n",
      "  í•˜ì°¨ìŠ¹ê°ìˆ˜: 0ê°œ\n",
      "\n",
      "ìŠ¹ê°ìˆ˜ í†µê³„:\n",
      "  í‰ê·  í•˜ì°¨ìŠ¹ê°ìˆ˜: 23459.7ëª…\n",
      "  ìµœëŒ€ í•˜ì°¨ìŠ¹ê°ìˆ˜: 116,745ëª…\n",
      "  ìµœì†Œ í•˜ì°¨ìŠ¹ê°ìˆ˜: 122ëª…\n",
      "\n",
      "ğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ê° í–‰ì •ë™ë³„ 1ê°œì”©):\n",
      "             ì—­ëª…      ì‚¬ìš©ì¼ì  í•˜ì°¨ì´ìŠ¹ê°ìˆ˜\n",
      "í–‰ì •ë™ì½”ë“œ                            \n",
      "11110615   ì¢…ë¡œ5ê°€  20240501   24236\n",
      "11110650    ë™ë¬˜ì•  20240501   13926\n",
      "11140550  ì„ì§€ë¡œì…êµ¬  20240501   33688\n",
      "11140605    ë™ëŒ€ë¬¸  20240501   13214\n",
      "11170520    ì‚¼ê°ì§€  20240501    6555\n",
      "11170650    ì´íƒœì›  20240501   11796\n",
      "11200660     ëšì„¬  20240501   19878\n",
      "11200690     ì„±ìˆ˜  20240501   38758\n",
      "11440660     í•©ì •  20240501   29691\n",
      "11440690     ìƒìˆ˜  20240501   11108\n",
      "11440710   í™ëŒ€ì…êµ¬  20240501   15051\n",
      "11560605  ì˜ë“±í¬êµ¬ì²­  20240501   14659\n",
      "11620745     ì‹ ë¦¼  20240501    2302\n",
      "11680510     ê°•ë‚¨  20240501   50462\n",
      "11680640     ì„ ë¦‰  20240501    8778\n",
      "11710580     ì„ì´Œ  20240501    8405\n"
     ]
    }
   ],
   "source": [
    "def check_subway_mapping_quality():\n",
    "    \"\"\"\n",
    "    ì§€í•˜ì²  ë°ì´í„° ë§¤í•‘ í’ˆì§ˆ ì²´í¬\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” ì§€í•˜ì²  ë°ì´í„° ë§¤í•‘ í’ˆì§ˆ ì²´í¬\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ì™„ì „í•œ 12ê°œì›” ë°ì´í„° ë¡œë“œ\n",
    "    subway_df = pd.read_csv(\"complete_12months_subway_data.csv\")\n",
    "    \n",
    "    print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° í˜„í™©:\")\n",
    "    print(f\"ì´ ë°ì´í„°: {len(subway_df):,}ê°œ í–‰\")\n",
    "    print(f\"ê³ ìœ  ì—­: {subway_df['ì—­ëª…'].nunique()}ê°œ\")\n",
    "    print(f\"ê³ ìœ  í–‰ì •ë™: {subway_df['í–‰ì •ë™ì½”ë“œ'].nunique()}ê°œ\")\n",
    "    print(f\"ë‚ ì§œ ë²”ìœ„: {subway_df['ì‚¬ìš©ì¼ì'].min()} ~ {subway_df['ì‚¬ìš©ì¼ì'].max()}\")\n",
    "    \n",
    "    # 1. í–‰ì •ë™ë³„ ë§¤í•‘ í˜„í™©\n",
    "    print(f\"\\nğŸ“‹ í–‰ì •ë™ë³„ ë§¤í•‘ í˜„í™©:\")\n",
    "    admin_mapping = subway_df.groupby('í–‰ì •ë™ì½”ë“œ').agg({\n",
    "        'ì—­ëª…': 'nunique',\n",
    "        'ì‚¬ìš©ì¼ì': 'nunique'\n",
    "    }).rename(columns={'ì—­ëª…': 'ì—­ìˆ˜', 'ì‚¬ìš©ì¼ì': 'ì¼ìˆ˜'})\n",
    "    \n",
    "    for admin_code, row in admin_mapping.iterrows():\n",
    "        print(f\"  {admin_code}: {row['ì—­ìˆ˜']}ê°œ ì—­, {row['ì¼ìˆ˜']}ì¼\")\n",
    "    \n",
    "    # 2. ì—­ë³„ ë°ì´í„° ë¶„í¬\n",
    "    print(f\"\\nğŸ“‹ ì—­ë³„ ë°ì´í„° ë¶„í¬:\")\n",
    "    station_counts = subway_df['ì—­ëª…'].value_counts()\n",
    "    for station, count in station_counts.items():\n",
    "        admin_code = subway_df[subway_df['ì—­ëª…'] == station]['í–‰ì •ë™ì½”ë“œ'].iloc[0]\n",
    "        print(f\"  {station} ({admin_code}): {count}ê°œ í–‰\")\n",
    "    \n",
    "    # 3. ì›”ë³„ ë°ì´í„° ë¶„í¬\n",
    "    print(f\"\\nğŸ“‹ ì›”ë³„ ë°ì´í„° ë¶„í¬:\")\n",
    "    subway_df['ì›”'] = subway_df['ì‚¬ìš©ì¼ì'].astype(str).str[4:6]\n",
    "    monthly_counts = subway_df['ì›”'].value_counts().sort_index()\n",
    "    for month, count in monthly_counts.items():\n",
    "        print(f\"  {month}ì›”: {count}ê°œ í–‰\")\n",
    "    \n",
    "    # 4. ë°ì´í„° í’ˆì§ˆ ì²´í¬\n",
    "    print(f\"\\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "    missing_station = subway_df['ì—­ëª…'].isnull().sum()\n",
    "    missing_admin = subway_df['í–‰ì •ë™ì½”ë“œ'].isnull().sum()\n",
    "    missing_inflow = subway_df['í•˜ì°¨ì´ìŠ¹ê°ìˆ˜'].isnull().sum()\n",
    "    \n",
    "    print(f\"ê²°ì¸¡ì¹˜:\")\n",
    "    print(f\"  ì—­ëª…: {missing_station}ê°œ\")\n",
    "    print(f\"  í–‰ì •ë™ì½”ë“œ: {missing_admin}ê°œ\")\n",
    "    print(f\"  í•˜ì°¨ìŠ¹ê°ìˆ˜: {missing_inflow}ê°œ\")\n",
    "    \n",
    "    # ì´ìƒê°’ í™•ì¸\n",
    "    print(f\"\\nìŠ¹ê°ìˆ˜ í†µê³„:\")\n",
    "    print(f\"  í‰ê·  í•˜ì°¨ìŠ¹ê°ìˆ˜: {subway_df['í•˜ì°¨ì´ìŠ¹ê°ìˆ˜'].mean():.1f}ëª…\")\n",
    "    print(f\"  ìµœëŒ€ í•˜ì°¨ìŠ¹ê°ìˆ˜: {subway_df['í•˜ì°¨ì´ìŠ¹ê°ìˆ˜'].max():,}ëª…\")\n",
    "    print(f\"  ìµœì†Œ í•˜ì°¨ìŠ¹ê°ìˆ˜: {subway_df['í•˜ì°¨ì´ìŠ¹ê°ìˆ˜'].min():,}ëª…\")\n",
    "    \n",
    "    # 5. ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "    print(f\"\\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ê° í–‰ì •ë™ë³„ 1ê°œì”©):\")\n",
    "    sample_data = subway_df.groupby('í–‰ì •ë™ì½”ë“œ').first()\n",
    "    print(sample_data[['ì—­ëª…', 'ì‚¬ìš©ì¼ì', 'í•˜ì°¨ì´ìŠ¹ê°ìˆ˜']])\n",
    "    \n",
    "    return subway_df\n",
    "\n",
    "# ì‹¤í–‰\n",
    "mapping_check = check_subway_mapping_quality()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7b9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘!\n",
      "ğŸ¯ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± (ìŠ¹ì°¨/í•˜ì°¨ ë°ì´í„° í¬í•¨)\n",
      "==================================================\n",
      "1ë‹¨ê³„: ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë“œ\n",
      "ë©”ì¸ ë°ì´í„°ì…‹: 5,840ê°œ í–‰\n",
      "ì»¬ëŸ¼: ['STDR_DE_ID', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'culture_event_count', 'ë¶„ë¥˜', 'ìœ ë¬´ë£Œ', 'has_culture_event', 'is_holiday', 'avg_temp', 'max_temp', 'min_temp', 'precipitation', 'wind_speed', 'sunshine_hours', 'humidity', 'ground_temp']\n",
      "\n",
      "2ë‹¨ê³„: ì§€í•˜ì²  ë°ì´í„° ë¡œë“œ\n",
      "ì§€í•˜ì²  ë°ì´í„°: 17,837ê°œ í–‰\n",
      "ê³ ìœ  ì—­: 31ê°œ\n",
      "ê³ ìœ  í–‰ì •ë™: 16ê°œ\n",
      "\n",
      "3ë‹¨ê³„: ì§€í•˜ì²  ë°ì´í„° ì§‘ê³„\n",
      "ì§‘ê³„ëœ ì§€í•˜ì²  ë°ì´í„°: 5,840ê°œ í–‰\n",
      "ê³ ìœ  ë‚ ì§œ: 365ê°œ\n",
      "ê³ ìœ  í–‰ì •ë™: 16ê°œ\n",
      "\n",
      "4ë‹¨ê³„: ë³‘í•© í‚¤ ìƒì„±\n",
      "ë©”ì¸ ë°ì´í„° í‚¤ ìƒ˜í”Œ:    date_key admin_key\n",
      "0  20240501  11110615\n",
      "1  20240501  11110650\n",
      "2  20240501  11140550\n",
      "3  20240501  11140605\n",
      "4  20240501  11170520\n",
      "ì§€í•˜ì²  ë°ì´í„° í‚¤ ìƒ˜í”Œ:    date_key admin_key\n",
      "0  20240501  11110615\n",
      "1  20240501  11110650\n",
      "2  20240501  11140550\n",
      "3  20240501  11140605\n",
      "4  20240501  11170520\n",
      "\n",
      "5ë‹¨ê³„: ë°ì´í„° ë³‘í•©\n",
      "ë³‘í•© í›„ ë°ì´í„°: 5,840ê°œ í–‰\n",
      "\n",
      "6ë‹¨ê³„: ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
      "ìœ ì… ê²°ì¸¡ì¹˜ ì²˜ë¦¬: 0ê°œ â†’ 0ê°œ\n",
      "ìœ ì¶œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬: 0ê°œ â†’ 0ê°œ\n",
      "\n",
      "7ë‹¨ê³„: ê²°ê³¼ í™•ì¸\n",
      "ğŸ“Š ìµœì¢… ë³‘í•© ê²°ê³¼:\n",
      "ì „ì²´ ë ˆì½”ë“œ: 5,840ê°œ\n",
      "ì§€í•˜ì²  ë°ì´í„°ê°€ ìˆëŠ” ë ˆì½”ë“œ: 5,840ê°œ\n",
      "ì§€í•˜ì²  ë°ì´í„° ë¹„ìœ¨: 100.00%\n",
      "í‰ê·  ì§€í•˜ì²  ìœ ì…ê°: 71652.6ëª…\n",
      "í‰ê·  ì§€í•˜ì²  ìœ ì¶œê°: 69750.5ëª…\n",
      "ìµœëŒ€ ì§€í•˜ì²  ìœ ì…ê°: 237,061ëª…\n",
      "ìµœëŒ€ ì§€í•˜ì²  ìœ ì¶œê°: 206,455ëª…\n",
      "\n",
      "ğŸ“‹ í–‰ì •ë™ë³„ ì§€í•˜ì²  ì´ìš© í˜„í™©:\n",
      "                    í‰ê· ìœ ì…    ìµœëŒ€ìœ ì…      í‰ê· ìœ ì¶œ    ìµœëŒ€ìœ ì¶œ\n",
      "ADSTRD_CODE_SE                                    \n",
      "11110615        143575.4  191291  146001.4  206455\n",
      "11110650         19394.4   29385   19099.2   29751\n",
      "11140550        104204.4  147915  100980.7  141497\n",
      "11140605         30993.4   39806   30738.3   42071\n",
      "11170520         69034.0  105275   66710.3  103345\n",
      "11170650         12801.4   22120   11635.2   22170\n",
      "11200660         76370.9   99537   73380.7   95787\n",
      "11200690         47572.9   65447   43613.3   61084\n",
      "11440660         47854.9   64957   45254.4   57617\n",
      "11440690         11454.6   15775   10189.5   13483\n",
      "11440710        151613.7  207820  144094.6  190800\n",
      "11560605        119152.2  237061  115947.1  187716\n",
      "11620745         75854.4   92389   78414.5   96380\n",
      "11680510         74030.3   99530   75321.4  100409\n",
      "11680640        144573.5  194317  136996.0  185736\n",
      "11710580         17961.4   30376   17631.6   26993\n",
      "\n",
      "8ë‹¨ê³„: ë°ì´í„° ì €ì¥\n",
      "ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ì´ 'final_dataset_with_subway.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“‹ ìµœì¢… ë°ì´í„°ì…‹ ì»¬ëŸ¼ (18ê°œ):\n",
      "   1. STDR_DE_ID\n",
      "   2. ADSTRD_CODE_SE\n",
      "   3. TOT_LVPOP_CO\n",
      "   4. culture_event_count\n",
      "   5. ë¶„ë¥˜\n",
      "   6. ìœ ë¬´ë£Œ\n",
      "   7. has_culture_event\n",
      "   8. is_holiday\n",
      "   9. avg_temp\n",
      "  10. max_temp\n",
      "  11. min_temp\n",
      "  12. precipitation\n",
      "  13. wind_speed\n",
      "  14. sunshine_hours\n",
      "  15. humidity\n",
      "  16. ground_temp\n",
      "  17. subway_inflow\n",
      "  18. subway_outflow\n",
      "\n",
      "ğŸ“‹ ìµœì¢… ë°ì´í„° ìƒ˜í”Œ:\n",
      "   STDR_DE_ID  ADSTRD_CODE_SE  TOT_LVPOP_CO  subway_inflow  subway_outflow  \\\n",
      "0    20240501        11110615    99785.2975         123392          130977   \n",
      "1    20240501        11110650    43377.5306          23113           22273   \n",
      "2    20240501        11140550    54393.7284          87651           88445   \n",
      "3    20240501        11140605    20673.6264          37771           36987   \n",
      "4    20240501        11170520    13714.7677          63437           60673   \n",
      "5    20240501        11170650    13215.9398          11796           12025   \n",
      "6    20240501        11200660    24900.6383          71512           71515   \n",
      "7    20240501        11200690    40082.2197          38758           34811   \n",
      "8    20240501        11440660   106161.9538          41878           40872   \n",
      "9    20240501        11440690    23856.3010          11108           10315   \n",
      "\n",
      "   is_holiday  has_culture_event  \n",
      "0           0                  0  \n",
      "1           0                  0  \n",
      "2           0                  0  \n",
      "3           0                  0  \n",
      "4           0                  0  \n",
      "5           0                  0  \n",
      "6           0                  0  \n",
      "7           0                  0  \n",
      "8           0                  0  \n",
      "9           0                  0  \n",
      "\n",
      "âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: (5840, 18)\n",
      "ğŸ“ˆ ìƒí™œì¸êµ¬ ì˜ˆì¸¡ì„ ìœ„í•œ ì™„ì „í•œ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "\n",
      "ğŸ¯ ìƒˆë¡œ ì¶”ê°€ëœ feature:\n",
      "  - subway_inflow: ì§€í•˜ì²  ìœ ì… ìŠ¹ê°ìˆ˜ (í•˜ì°¨)\n",
      "  - subway_outflow: ì§€í•˜ì²  ìœ ì¶œ ìŠ¹ê°ìˆ˜ (ìŠ¹ì°¨)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_final_dataset_with_subway():\n",
    "    \"\"\"\n",
    "    ìŠ¹ì°¨/í•˜ì°¨ ìŠ¹ê°ìˆ˜ë¡œ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± (ìŠ¹ì°¨/í•˜ì°¨ ë°ì´í„° í¬í•¨)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    print(\"1ë‹¨ê³„: ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë“œ\")\n",
    "    main_df = pd.read_csv(\"dataset.csv\")\n",
    "    print(f\"ë©”ì¸ ë°ì´í„°ì…‹: {len(main_df):,}ê°œ í–‰\")\n",
    "    print(f\"ì»¬ëŸ¼: {main_df.columns.tolist()}\")\n",
    "    \n",
    "    # 2. ì™„ì „í•œ 12ê°œì›” ì§€í•˜ì²  ë°ì´í„° ë¡œë“œ\n",
    "    print(\"\\n2ë‹¨ê³„: ì§€í•˜ì²  ë°ì´í„° ë¡œë“œ\")\n",
    "    subway_df = pd.read_csv(\"complete_12months_subway_data.csv\")\n",
    "    print(f\"ì§€í•˜ì²  ë°ì´í„°: {len(subway_df):,}ê°œ í–‰\")\n",
    "    print(f\"ê³ ìœ  ì—­: {subway_df['ì—­ëª…'].nunique()}ê°œ\")\n",
    "    print(f\"ê³ ìœ  í–‰ì •ë™: {subway_df['í–‰ì •ë™ì½”ë“œ'].nunique()}ê°œ\")\n",
    "    \n",
    "    # 3. ì§€í•˜ì²  ë°ì´í„°ë¥¼ í–‰ì •ë™ë³„, ë‚ ì§œë³„ë¡œ ì§‘ê³„ (ìŠ¹ì°¨/í•˜ì°¨ ëª¨ë‘)\n",
    "    print(\"\\n3ë‹¨ê³„: ì§€í•˜ì²  ë°ì´í„° ì§‘ê³„\")\n",
    "    subway_agg = subway_df.groupby(['ì‚¬ìš©ì¼ì', 'í–‰ì •ë™ì½”ë“œ']).agg({\n",
    "        'í•˜ì°¨ì´ìŠ¹ê°ìˆ˜': 'sum',  # ìœ ì… (í•´ë‹¹ ì§€ì—­ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì‚¬ëŒ)\n",
    "        'ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜': 'sum'   # ìœ ì¶œ (í•´ë‹¹ ì§€ì—­ì—ì„œ ë‚˜ê°€ëŠ” ì‚¬ëŒ)\n",
    "    }).reset_index()\n",
    "    \n",
    "    subway_agg.rename(columns={\n",
    "        'í•˜ì°¨ì´ìŠ¹ê°ìˆ˜': 'subway_inflow',   # ìœ ì…\n",
    "        'ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜': 'subway_outflow'   # ìœ ì¶œ\n",
    "    }, inplace=True)\n",
    "    \n",
    "    print(f\"ì§‘ê³„ëœ ì§€í•˜ì²  ë°ì´í„°: {len(subway_agg):,}ê°œ í–‰\")\n",
    "    print(f\"ê³ ìœ  ë‚ ì§œ: {subway_agg['ì‚¬ìš©ì¼ì'].nunique()}ê°œ\")\n",
    "    print(f\"ê³ ìœ  í–‰ì •ë™: {subway_agg['í–‰ì •ë™ì½”ë“œ'].nunique()}ê°œ\")\n",
    "    \n",
    "    # 4. ë³‘í•© í‚¤ ìƒì„±\n",
    "    print(\"\\n4ë‹¨ê³„: ë³‘í•© í‚¤ ìƒì„±\")\n",
    "    main_df['date_key'] = main_df['STDR_DE_ID'].astype(str)\n",
    "    main_df['admin_key'] = main_df['ADSTRD_CODE_SE'].astype(str)\n",
    "    subway_agg['date_key'] = subway_agg['ì‚¬ìš©ì¼ì'].astype(str)\n",
    "    subway_agg['admin_key'] = subway_agg['í–‰ì •ë™ì½”ë“œ'].astype(str)\n",
    "    \n",
    "    print(f\"ë©”ì¸ ë°ì´í„° í‚¤ ìƒ˜í”Œ: {main_df[['date_key', 'admin_key']].head()}\")\n",
    "    print(f\"ì§€í•˜ì²  ë°ì´í„° í‚¤ ìƒ˜í”Œ: {subway_agg[['date_key', 'admin_key']].head()}\")\n",
    "    \n",
    "    # 5. Left Join ì‹¤í–‰\n",
    "    print(\"\\n5ë‹¨ê³„: ë°ì´í„° ë³‘í•©\")\n",
    "    merged_df = main_df.merge(\n",
    "        subway_agg[['date_key', 'admin_key', 'subway_inflow', 'subway_outflow']],\n",
    "        on=['date_key', 'admin_key'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"ë³‘í•© í›„ ë°ì´í„°: {len(merged_df):,}ê°œ í–‰\")\n",
    "    \n",
    "    # 6. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "    print(\"\\n6ë‹¨ê³„: ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
    "    before_fill_inflow = merged_df['subway_inflow'].isnull().sum()\n",
    "    before_fill_outflow = merged_df['subway_outflow'].isnull().sum()\n",
    "    \n",
    "    merged_df['subway_inflow'] = merged_df['subway_inflow'].fillna(0).astype(int)\n",
    "    merged_df['subway_outflow'] = merged_df['subway_outflow'].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"ìœ ì… ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {before_fill_inflow}ê°œ â†’ 0ê°œ\")\n",
    "    print(f\"ìœ ì¶œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {before_fill_outflow}ê°œ â†’ 0ê°œ\")\n",
    "    \n",
    "    # 7. ì„ì‹œ ì»¬ëŸ¼ ì œê±°\n",
    "    merged_df = merged_df.drop(['date_key', 'admin_key'], axis=1)\n",
    "    \n",
    "    # 8. ê²°ê³¼ í™•ì¸\n",
    "    print(\"\\n7ë‹¨ê³„: ê²°ê³¼ í™•ì¸\")\n",
    "    total_records = len(merged_df)\n",
    "    records_with_subway = len(merged_df[merged_df['subway_inflow'] > 0])\n",
    "    \n",
    "    print(f\"ğŸ“Š ìµœì¢… ë³‘í•© ê²°ê³¼:\")\n",
    "    print(f\"ì „ì²´ ë ˆì½”ë“œ: {total_records:,}ê°œ\")\n",
    "    print(f\"ì§€í•˜ì²  ë°ì´í„°ê°€ ìˆëŠ” ë ˆì½”ë“œ: {records_with_subway:,}ê°œ\")\n",
    "    print(f\"ì§€í•˜ì²  ë°ì´í„° ë¹„ìœ¨: {records_with_subway/total_records*100:.2f}%\")\n",
    "    print(f\"í‰ê·  ì§€í•˜ì²  ìœ ì…ê°: {merged_df['subway_inflow'].mean():.1f}ëª…\")\n",
    "    print(f\"í‰ê·  ì§€í•˜ì²  ìœ ì¶œê°: {merged_df['subway_outflow'].mean():.1f}ëª…\")\n",
    "    print(f\"ìµœëŒ€ ì§€í•˜ì²  ìœ ì…ê°: {merged_df['subway_inflow'].max():,}ëª…\")\n",
    "    print(f\"ìµœëŒ€ ì§€í•˜ì²  ìœ ì¶œê°: {merged_df['subway_outflow'].max():,}ëª…\")\n",
    "    \n",
    "    # 9. í–‰ì •ë™ë³„ ì§€í•˜ì²  ì´ìš© í˜„í™©\n",
    "    print(f\"\\nğŸ“‹ í–‰ì •ë™ë³„ ì§€í•˜ì²  ì´ìš© í˜„í™©:\")\n",
    "    admin_stats = merged_df.groupby('ADSTRD_CODE_SE').agg({\n",
    "        'subway_inflow': ['mean', 'max'],\n",
    "        'subway_outflow': ['mean', 'max']\n",
    "    }).round(1)\n",
    "    admin_stats.columns = ['í‰ê· ìœ ì…', 'ìµœëŒ€ìœ ì…', 'í‰ê· ìœ ì¶œ', 'ìµœëŒ€ìœ ì¶œ']\n",
    "    print(admin_stats)\n",
    "    \n",
    "    # 10. ìµœì¢… ë°ì´í„° ì €ì¥\n",
    "    print(\"\\n8ë‹¨ê³„: ë°ì´í„° ì €ì¥\")\n",
    "    output_file = \"final_dataset_with_subway.csv\"\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ì´ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 11. ìµœì¢… ì»¬ëŸ¼ í™•ì¸\n",
    "    print(f\"\\nğŸ“‹ ìµœì¢… ë°ì´í„°ì…‹ ì»¬ëŸ¼ ({len(merged_df.columns)}ê°œ):\")\n",
    "    for i, col in enumerate(merged_df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    # 12. ìƒ˜í”Œ ë°ì´í„°\n",
    "    print(f\"\\nğŸ“‹ ìµœì¢… ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "    sample_cols = ['STDR_DE_ID', 'ADSTRD_CODE_SE', 'TOT_LVPOP_CO', 'subway_inflow', 'subway_outflow', 'is_holiday', 'has_culture_event']\n",
    "    print(merged_df[sample_cols].head(10))\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"ğŸš€ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘!\")\n",
    "final_dataset = create_final_dataset_with_subway()\n",
    "\n",
    "if final_dataset is not None:\n",
    "    print(\"\\nâœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {final_dataset.shape}\")\n",
    "    print(\"ğŸ“ˆ ìƒí™œì¸êµ¬ ì˜ˆì¸¡ì„ ìœ„í•œ ì™„ì „í•œ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(\"\\nğŸ¯ ìƒˆë¡œ ì¶”ê°€ëœ feature:\")\n",
    "    print(\"  - subway_inflow: ì§€í•˜ì²  ìœ ì… ìŠ¹ê°ìˆ˜ (í•˜ì°¨)\")\n",
    "    print(\"  - subway_outflow: ì§€í•˜ì²  ìœ ì¶œ ìŠ¹ê°ìˆ˜ (ìŠ¹ì°¨)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62283cfa",
   "metadata": {},
   "source": [
    "#### ë²„ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8c1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” 7ìë¦¬ ì½”ë“œ ë§¤ì¹­ ì‹œë„\n",
      "==================================================\n",
      "ì¡°ì‚¬ ëŒ€ìƒ 8ìë¦¬ ì½”ë“œ: 16ê°œ\n",
      "ì¡°ì‚¬ ëŒ€ìƒ 7ìë¦¬ ì½”ë“œ: 16ê°œ\n",
      "\n",
      "ğŸ“Š 7ìë¦¬ ë§¤ì¹­ ê²°ê³¼:\n",
      "ë§¤ì¹­ëœ ë°ì´í„°: 8,701ê°œ í–‰\n",
      "ë§¤ì¹­ëœ ê³ ìœ  7ìë¦¬ ì½”ë“œ: 7ê°œ\n",
      "\n",
      "âœ… ë§¤ì¹­ëœ 7ìë¦¬ ì½”ë“œ (7ê°œ):\n",
      "  1111061 â†’ ['11110615']\n",
      "  1111065 â†’ ['11110650']\n",
      "  1114060 â†’ ['11140605']\n",
      "  1117052 â†’ ['11170520']\n",
      "  1117065 â†’ ['11170650']\n",
      "  1120066 â†’ ['11200660']\n",
      "  1120069 â†’ ['11200690']\n",
      "\n",
      "âŒ ë§¤ì¹­ ì•ˆëœ 7ìë¦¬ ì½”ë“œ (9ê°œ):\n",
      "  1114055 â†’ ['11140550']\n",
      "  1144066 â†’ ['11440660']\n",
      "  1144069 â†’ ['11440690']\n",
      "  1144071 â†’ ['11440710']\n",
      "  1156060 â†’ ['11560605']\n",
      "  1162074 â†’ ['11620745']\n",
      "  1168051 â†’ ['11680510']\n",
      "  1168064 â†’ ['11680640']\n",
      "  1171058 â†’ ['11710580']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89086074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ºï¸ í™•ì¥ëœ ë§¤í•‘ í…Œì´ë¸” ìƒì„±\n",
      "==================================================\n",
      "ğŸ” ë§¤ì¹­ ì•ˆëœ í–‰ì •ë™ ì‹¬ì¸µ ë¶„ì„\n",
      "==================================================\n",
      "ğŸ“‹ ë²„ìŠ¤ ë°ì´í„°ì˜ ëª¨ë“  ê³ ìœ  í–‰ì •ë™ ì½”ë“œ:\n",
      "\n",
      "ğŸ” 1114055 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "  ğŸ“‹ 6ìë¦¬ ìœ ì‚¬: ['1114059', '11140590']...\n",
      "  ğŸ“‹ 5ìë¦¬ ìœ ì‚¬: ['1114059', '11140590', '1114060', '11140600', '1114061']...\n",
      "\n",
      "ğŸ” 1144066 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” 1144069 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” 1144071 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” 1156060 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” 1162074 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” 1168051 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” 1168064 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” 1171058 ê´€ë ¨ ì½”ë“œ ì°¾ê¸°:\n",
      "\n",
      "ğŸ” íŠ¹ë³„ íŒ¨í„´ ë¶„ì„:\n",
      "ê°•ë‚¨êµ¬(1168) ê´€ë ¨ ì½”ë“œ: []\n",
      "ë§ˆí¬êµ¬(1144) ê´€ë ¨ ì½”ë“œ: []\n",
      "ì˜ë“±í¬êµ¬(1156) ê´€ë ¨ ì½”ë“œ: []\n",
      "ê¸°ì¡´ ë§¤í•‘: 7ê°œ\n",
      "í™•ì¥ í›„ ë§¤í•‘: 7ê°œ\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9f619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
